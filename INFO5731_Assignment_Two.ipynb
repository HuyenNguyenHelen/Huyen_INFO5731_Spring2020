{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/Huyen_INFO5731_Spring2020/blob/master/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPjGGLvpVGE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzMjKmiPTHlH",
        "colab_type": "code",
        "outputId": "ca377184-5ba0-4b42-e3d2-152704ba72f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Question1\n",
        "\n",
        "#Yesss I fixed that problem\n",
        "\n",
        "host_url = [\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"]\n",
        "load_more_urls = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "all_urls = host_url+load_more_urls\n",
        "user_names, user_links, content, date, title, review_helpfulness = [],[], [], [], [], []\n",
        "review_elements = []\n",
        "for url in all_urls:\n",
        "  request = urllib.request.urlopen (url)\n",
        "  webpage  = request.read()\n",
        "  soup = BeautifulSoup(webpage)\n",
        "\n",
        "  reviews = soup.find_all(\"div\", {\"class\":\"lister-item-content\" })\n",
        "  for review in reviews:\n",
        "    try:\n",
        "        review_title = review.find(\"a\", class_=\"title\").text.strip()\n",
        "    except:\n",
        "        review_title = \"\"\n",
        "    try:\n",
        "        review_id = review.find(\"a\")[\"href\"].split(\"/\")[2]\n",
        "    except:\n",
        "        review_id = \"\"\n",
        "    try:\n",
        "        review_date = review.find(\"span\", class_=\"review-date\").get_text()\n",
        "    except:\n",
        "        review_date = \"\"\n",
        "    try:\n",
        "        review_nickname = review.find (\"span\", class_=\"display-name-link\").get_text()\n",
        "    except:\n",
        "        review_nickname = \"\"\n",
        "    try:\n",
        "        review_content = review.find (\"div\", class_=\"text show-more__control\").get_text()\n",
        "    except:\n",
        "        review_content =\"\"\n",
        "    #print(review_content)\n",
        "    try:\n",
        "        review_rating = review. find (\"span\", class_=\"rating-other-user-rating\").getText().strip()\n",
        "        #print(review_rating)\n",
        "    except:\n",
        "        review_rating = \"\"\n",
        "    try:\n",
        "        review_helpfulness = review.find(\"div\", {\"class\": \"actions text-muted\"}).text.split(\"\\n\")[1]\n",
        "        #print(review_helpfulness)\n",
        "    except:\n",
        "        review_helpfulness = \"\"\n",
        "\n",
        "    review_elements.append([review_title, review_id, review_date, review_nickname, review_content, review_rating, review_helpfulness])\n",
        "#print(len(review_elements))\n",
        "\n",
        "df = pd.DataFrame(review_elements, columns=[\"review_title\", \"review_id\", \"review_date\", \"review_nickname\", \"review_content\", \"review_rating\", \"review_helpfulness\"])\n",
        "df\n",
        "\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"w\", newline=\"\", encoding='utf-8') as file:\n",
        "  df.to_csv(file)\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"r\", encoding='utf-8') as file:\n",
        "  pd.read_csv(file)\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6v-2mpvil47",
        "colab_type": "code",
        "outputId": "1ebdf538-938f-4b05-bc95-4380ead40255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#try_codes\n",
        "load_more_data = soup.find_all(\"div\", attrs={\"class\": \"load-more-data\"}) # find this button at the end of each page\n",
        "#print(load_more_data)\n",
        "for item in load_more_data:\n",
        "  path_parse1 = \"https://www.imdb.com\"\n",
        "  path_parse2 = \"/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=\"\n",
        "  path_parse3 = item.get(\"data-key\")\n",
        "  concatenated_path = path_parse1+path_parse2+path_parse3\n",
        "  print(concatenated_path)\n",
        "#load_more_links = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "#for link in load_more_links:\n",
        " # print(link) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqu2ta3qb7oux7nrvqpr42brhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k4rmwb65sqjd2c5p4ic5kj7njmla\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBytY15j3Hkx",
        "colab_type": "code",
        "outputId": "7cfe0751-0961-4169-9932-3398d2e6baee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "# In this line, I am attempting to generate links with data-key in the class load-more-data. However, because, the server prohibits to get key in the second next page, so my code raise error.\n",
        "host_url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "def link_load_more (host_url):\n",
        "  request = urllib.request.urlopen (host_url)\n",
        "  webpage  = request.read()\n",
        "  soup = BeautifulSoup(webpage)\n",
        "  load_more_data = soup.find_all(\"div\", attrs={\"class\": \"load-more-data\"}) # find this button at the end of each page\n",
        "  flag = True\n",
        "  if len (load_more_data):\n",
        "  #print(load_more_data)\n",
        "    for item in load_more_data:\n",
        "      path_parse1 = \"https://www.imdb.com/\"\n",
        "      path_parse2 = \"title/tt7286456/reviews/_ajax\"\n",
        "      path_parse3 = \"?ref_=undefined&paginationKey=\"\n",
        "      try:\n",
        "        path_parse4 = item.get(\"data-key\")\n",
        "        concatenated_path = path_parse1+path_parse2+path_parse3+path_parse4\n",
        "      except KeyError:\n",
        "        flag = False\n",
        "  return concatenated_path\n",
        "\n",
        "# generate link and parse all of four pages\n",
        "\n",
        "url_nextpage1 = link_load_more(host_url)\n",
        "url_nextpage2 = link_load_more(url_nextpage1)\n",
        "url_nextpage3 = link_load_more(url_nextpage2)\n",
        "url_nextpage4 = link_load_more(url_nextpage3)\n",
        "\n",
        "print(url_nextpage1)\n",
        "print(url_nextpage2)\n",
        "print(url_nextpage3)\n",
        "print(url_nextpage4)\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-79216aae5d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0murl_nextpage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0murl_nextpage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_nextpage1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0murl_nextpage3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_nextpage2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0murl_nextpage4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_nextpage3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-79216aae5d21>\u001b[0m in \u001b[0;36mlink_load_more\u001b[0;34m(host_url)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenated_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# generate link and parse all of four pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'concatenated_path' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "outputId": "1e4133d6-82f2-4ae7-b10e-e9d017d0b8aa"
      },
      "source": [
        "# Write your code here\n",
        "#clean the text data\n",
        "from __future__ import division\n",
        "import nltk # re, pprint\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string \n",
        "\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"r\", encoding='utf-8') as file:\n",
        "  data = pd.read_csv(file)\n",
        "\n",
        "raw_title = [str(line) for line in data.review_title]\n",
        "raw_content = [str(line) for line in data.review_content]\n",
        "# Tokenize raw text in raw_title and raw_content\n",
        "tokens_title= [nltk.word_tokenize(title) for title in raw_title]\n",
        "tokens_content = [nltk.word_tokenize(item) for item in raw_content]\n",
        "\n",
        "# PROCCESSES OF CLEANING TEXT  \n",
        "def cleaning_text (text):\n",
        "    step1 = remove_noise(text)\n",
        "    step2 = remove_num (step1)\n",
        "    step3 = remove_stop (step2)\n",
        "    step4 = lowercase (step3)\n",
        "    step5 = stemming (step4)\n",
        "    step6 = lemma (step5)\n",
        "    return step6\n",
        "\n",
        "# (1) Remove noise, such as special characters and punctuations.      \n",
        "def remove_noise (list):\n",
        "    punct=[char for char in string.punctuation]\n",
        "    noise_removed = [word for word in list if word not in punct]\n",
        "    return noise_removed\n",
        "\n",
        "#(2) Remove numbers.\n",
        "def remove_num (list):\n",
        "    num_removed  = [word for word in list if not word.isnumeric()]\n",
        "    return num_removed\n",
        "\n",
        " # (3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "def remove_stop (list):\n",
        "    stopword_removed = [word for word in list if word not in stopwords.words('english')]\n",
        "    return stopword_removed\n",
        "\n",
        "#(4) Lowercase all texts\n",
        "def lowercase (list):\n",
        "    lowercase = [word.lower() for word in list]\n",
        "    return lowercase\n",
        "# (5) Stemming\n",
        "def stemming (list):\n",
        "    ps = PorterStemmer()\n",
        "    word_stemmed = [ps.stem(word) for word in list]\n",
        "    return word_stemmed\n",
        "# (6) Lemmatization.\n",
        "def lemma (list):\n",
        "    wnl =  WordNetLemmatizer ()\n",
        "    word_lemmatized = [wnl.lemmatize(word) for word in list]\n",
        "    return word_lemmatized\n",
        "\n",
        "# Cleaning title reviews and content review\n",
        "cleaned_title = [cleaning_text (sublist) for sublist in tokens_title]\n",
        "cleaned_content = [cleaning_text (sublist) for sublist in tokens_content]\n",
        "# Save cleaned text into new columns in dataframe\n",
        "df[\"cleaned_title\"] = cleaned_title\n",
        "df[\"cleaned_content\"] = cleaned_content\n",
        "df\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_title</th>\n",
              "      <th>review_id</th>\n",
              "      <th>review_date</th>\n",
              "      <th>review_nickname</th>\n",
              "      <th>review_content</th>\n",
              "      <th>review_rating</th>\n",
              "      <th>review_helpfulness</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a viewer that actually went to TIFF and wit...</td>\n",
              "      <td>rw5112402</td>\n",
              "      <td>10 September 2019</td>\n",
              "      <td>JF500</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>6,837 out of 7,858 found t...</td>\n",
              "      <td>[a, viewer, actual, went, tiff, wit, film, n't...</td>\n",
              "      <td>[i, person, saw, hype, claim, masterpiec, over...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outstanding movie with a haunting performance ...</td>\n",
              "      <td>rw5159304</td>\n",
              "      <td>3 October 2019</td>\n",
              "      <td>MihaVrhunc</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>3,375 out of 4,039 found t...</td>\n",
              "      <td>[outstand, movi, haunt, perform, best, charact...</td>\n",
              "      <td>[everi, movi, come, truli, make, impact, joaqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Only certain people can relate</td>\n",
              "      <td>rw5168360</td>\n",
              "      <td>7 October 2019</td>\n",
              "      <td>lesterarnoldpinto</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>2,673 out of 3,299 found t...</td>\n",
              "      <td>[onli, certain, peopl, relat]</td>\n",
              "      <td>[thi, movi, felt, alon, isol, truli, relat, yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Must have put a SMILE of Satisfaction on Heath...</td>\n",
              "      <td>rw5092831</td>\n",
              "      <td>1 September 2019</td>\n",
              "      <td>Chandler_Bing_</td>\n",
              "      <td>Truly a masterpiece, The Best film of 2019, on...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>3,166 out of 3,934 found t...</td>\n",
              "      <td>[must, put, smile, satisfact, heath, ledger, '...</td>\n",
              "      <td>[truli, masterpiec, the, best, film, one, best...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Hype is real</td>\n",
              "      <td>rw5160204</td>\n",
              "      <td>4 October 2019</td>\n",
              "      <td>kdagoulis26</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>2,352 out of 2,918 found t...</td>\n",
              "      <td>[the, hype, real]</td>\n",
              "      <td>[most, time, movi, anticip, like, end, fall, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>So disappointed by this film</td>\n",
              "      <td>rw5187643</td>\n",
              "      <td>14 October 2019</td>\n",
              "      <td>huntj-59716</td>\n",
              "      <td>Was really looking forward to seeing this film...</td>\n",
              "      <td>2/10</td>\n",
              "      <td>71 out of 139 found this h...</td>\n",
              "      <td>[so, disappoint, film]</td>\n",
              "      <td>[wa, realli, look, forward, see, film, howev, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Well filmed but empty</td>\n",
              "      <td>rw5179068</td>\n",
              "      <td>11 October 2019</td>\n",
              "      <td>Red_Identity</td>\n",
              "      <td>What can I say about it that hasn't already be...</td>\n",
              "      <td></td>\n",
              "      <td>41 out of 76 found this he...</td>\n",
              "      <td>[well, film, empti]</td>\n",
              "      <td>[what, i, say, n't, alreadi, said, it, 's, ter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Spare me the \"best comic book movie ever\" nons...</td>\n",
              "      <td>rw5144576</td>\n",
              "      <td>26 September 2019</td>\n",
              "      <td>gatozangado</td>\n",
              "      <td>It premiered last night in Sydney and I was lu...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>102 out of 206 found this ...</td>\n",
              "      <td>[spare, ``, best, comic, book, movi, ever, '',...</td>\n",
              "      <td>[it, premier, last, night, sydney, i, lucki, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>As movie well overrated - acting excellent</td>\n",
              "      <td>rw5175282</td>\n",
              "      <td>9 October 2019</td>\n",
              "      <td>dhllon</td>\n",
              "      <td>The depth of the story was lacking to much foc...</td>\n",
              "      <td>2/10</td>\n",
              "      <td>59 out of 114 found this h...</td>\n",
              "      <td>[a, movi, well, overr, act, excel]</td>\n",
              "      <td>[the, depth, stori, lack, much, focu, short, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>Lives up to the hype</td>\n",
              "      <td>rw5158293</td>\n",
              "      <td>3 October 2019</td>\n",
              "      <td>i_like_jellycups</td>\n",
              "      <td>A breath of fresh airThink heath ledgers joker...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>99 out of 200 found this h...</td>\n",
              "      <td>[live, hype]</td>\n",
              "      <td>[a, breath, fresh, airthink, heath, ledger, jo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>115 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          review_title  ...                                    cleaned_content\n",
              "0    As a viewer that actually went to TIFF and wit...  ...  [i, person, saw, hype, claim, masterpiec, over...\n",
              "1    Outstanding movie with a haunting performance ...  ...  [everi, movi, come, truli, make, impact, joaqu...\n",
              "2                       Only certain people can relate  ...  [thi, movi, felt, alon, isol, truli, relat, yo...\n",
              "3    Must have put a SMILE of Satisfaction on Heath...  ...  [truli, masterpiec, the, best, film, one, best...\n",
              "4                                     The Hype is real  ...  [most, time, movi, anticip, like, end, fall, s...\n",
              "..                                                 ...  ...                                                ...\n",
              "110                       So disappointed by this film  ...  [wa, realli, look, forward, see, film, howev, ...\n",
              "111                              Well filmed but empty  ...  [what, i, say, n't, alreadi, said, it, 's, ter...\n",
              "112  Spare me the \"best comic book movie ever\" nons...  ...  [it, premier, last, night, sydney, i, lucki, e...\n",
              "113         As movie well overrated - acting excellent  ...  [the, depth, stori, lack, much, focu, short, a...\n",
              "114                               Lives up to the hype  ...  [a, breath, fresh, airthink, heath, ledger, jo...\n",
              "\n",
              "[115 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "322184ab-c23a-4e95-d2fa-9626bcea8082"
      },
      "source": [
        "# Write your code here\n",
        "#(1) Tag parts of Speech (POS) and  calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb)...\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from collections import Counter\n",
        "hist = dict()\n",
        "count_POStag = []\n",
        "for tokens in cleaned_title:\n",
        "    title_tagged = nltk.pos_tag(tokens)\n",
        "    #fd_title = nltk.FreqDist(tag for (token, tag) in title_tagged)\n",
        "    #print(fd_title)\n",
        "    count_POStag.append(Counter (tag for tokens, tag in title_tagged))\n",
        "for tokens in cleaned_content:\n",
        "    content_tagged = nltk.pos_tag(tokens)\n",
        "    #fd_title = nltk.FreqDist(tag for (token, tag) in title_tagged)\n",
        "    #print(fd_title)\n",
        "    count_POStag. append (Counter (tag for tokens, tag in content_tagged))\n",
        "    \n",
        "print(len(count_POStag))\n",
        "print(count_POStag)\n",
        "count_VB=0\n",
        "count_NN=0\n",
        "count_ADJ=0\n",
        "count_ADV=0\n",
        "for pack in count_POStag:\n",
        "  for item in pack:\n",
        "    if item ==\"VB\":\n",
        "      count_VB+=1\n",
        "    elif item ==\"NN\":\n",
        "      count_NN+=1\n",
        "    elif item ==\"JJ\":\n",
        "      count_ADJ+=1\n",
        "    elif item ==\"ADV\":\n",
        "      count_ADV+=1\n",
        "    else:\n",
        "      pass\n",
        "print(\"The total verbs:\", count_VB)\n",
        "print(\"The total nouns:\", count_NN)\n",
        "print(\"The total adjectives:\", count_ADJ)\n",
        "print(\"The total adverbs:\", count_ADV)\n",
        "#for n in count_POStag:\n",
        " # for items in count_POStag\n",
        " # hist[key]=hist.get(valu=)+1\n",
        "  \n",
        "# (2)Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences.\n",
        "\n",
        "\n",
        "\n",
        "# for item in title_tagged_counts:\n",
        "      #print(item)\n",
        "     # hist[item]=hist.get(item)+1\n",
        "#print(hist\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "230\n",
            "[Counter({'NN': 9, 'JJ': 3, 'DT': 1, 'VBD': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 5, 'JJS': 1, 'VB': 1, 'RB': 1, 'VBN': 1}), Counter({'NN': 3, 'JJ': 1}), Counter({'NN': 4, 'RB': 2, 'MD': 1, 'VB': 1, 'JJ': 1, 'POS': 1}), Counter({'DT': 1, 'NN': 1, 'JJ': 1}), Counter({'NN': 2}), Counter({'RB': 1, 'VB': 1, 'WRB': 1, 'JJ': 1, 'VBP': 1}), Counter({'NN': 2, 'VBD': 1, 'JJ': 1}), Counter({'NN': 3, 'DT': 1, 'RB': 1, 'VBD': 1}), Counter({'NN': 6, 'POS': 1, 'JJS': 1, 'VBZ': 1, ':': 1, 'VBD': 1}), Counter({'JJ': 1, 'NN': 1}), Counter({'JJ': 2, 'NN': 1}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 2}), Counter({'NN': 4, 'RB': 2, 'VB': 1}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 2}), Counter({'VB': 2, 'VBP': 1, 'RB': 1}), Counter({'NN': 2, 'MD': 1, 'VB': 1}), Counter({'NN': 2}), Counter({'NN': 3, 'CD': 1, 'JJS': 1, 'VBP': 1, 'RB': 1, 'VBN': 1}), Counter({'IN': 1}), Counter({'NN': 2}), Counter({'IN': 1, 'DT': 1, 'NN': 1}), Counter({'NN': 1}), Counter({'NN': 1}), Counter({'NN': 2, 'DT': 1}), Counter({'JJ': 2, 'VBZ': 1, 'RB': 1, 'VB': 1}), Counter({'``': 1, 'DT': 1, 'VBZ': 1, 'NN': 1, \"''\": 1}), Counter({'NN': 2, 'JJ': 1}), Counter({'NN': 2}), Counter({'NN': 3, 'CC': 1, 'JJS': 1, 'VBP': 1, 'RB': 1, 'VBN': 1}), Counter({'NN': 2}), Counter({'JJ': 3, 'VBP': 2, 'NN': 2, 'IN': 1}), Counter({'RB': 2, 'NN': 2, 'JJ': 1, 'POS': 1, '``': 1, 'JJS': 1, \"''\": 1}), Counter({'NN': 3, 'JJ': 1, 'IN': 1}), Counter({'NN': 5, 'JJ': 2, 'VB': 1, 'CD': 1, 'JJS': 1, 'NNS': 1, 'VBP': 1}), Counter({'NN': 3, 'VBP': 2, 'VBD': 1, 'RB': 1, 'JJ': 1}), Counter({'JJ': 4, 'NN': 4, 'VBP': 3, 'VBN': 2, 'RB': 2, 'NNS': 1, 'IN': 1, 'PRP': 1}), Counter({'NN': 3, 'JJ': 1, '``': 1, \"''\": 1}), Counter({'NN': 3, 'JJ': 1, 'DT': 1}), Counter({'NN': 1}), Counter({'NN': 5, 'VBP': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 3, 'JJ': 1, 'VBP': 1, 'IN': 1}), Counter({'NNS': 1, 'NN': 1}), Counter({'JJ': 1, 'NN': 1}), Counter({'JJ': 3, 'NN': 2}), Counter({'NN': 5, 'VBD': 1}), Counter({'NN': 2, 'VBZ': 1, 'WRB': 1, 'DT': 1, 'JJ': 1, 'MD': 1, 'VB': 1}), Counter({'NN': 4, 'VBD': 2, 'JJS': 1, 'DT': 1}), Counter({'NN': 2, 'VB': 1}), Counter({'WP': 1, 'VBZ': 1, 'JJ': 1, 'NN': 1}), Counter({'NN': 2}), Counter({'NN': 4, 'RB': 2, 'JJ': 2, 'NNS': 1, 'VBP': 1}), Counter({'NN': 2, 'JJS': 1, 'IN': 1, 'DT': 1}), Counter({'NN': 2}), Counter({'NN': 2, 'VB': 2, 'RB': 1, 'JJ': 1, 'VBP': 1}), Counter({'NN': 1, 'POS': 1, 'PRP': 1}), Counter({'RB': 2, ':': 1}), Counter({'RB': 1, 'JJ': 1}), Counter({'VBP': 1, 'RB': 1, 'VB': 1, 'NN': 1}), Counter({'JJ': 3, 'NN': 2, ':': 1, 'POS': 1}), Counter({'NN': 4, 'JJ': 2}), Counter({'NN': 2, 'RB': 1}), Counter({'NN': 1}), Counter({'NN': 1}), Counter({'NN': 3, 'JJ': 1}), Counter({'NN': 4, 'DT': 1, 'JJS': 1, 'VBN': 1, 'VBD': 1}), Counter({'NN': 1, 'VBP': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 1}), Counter({'NN': 4, 'VBD': 1, 'IN': 1}), Counter({'NN': 2}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 1}), Counter({'RB': 1, 'NN': 1}), Counter({'NN': 2, 'JJ': 1}), Counter({'NN': 2, 'JJ': 1}), Counter({'NN': 2, 'JJ': 1}), Counter({'NN': 2}), Counter({'NN': 6, 'IN': 1, 'JJS': 1}), Counter({'NN': 2}), Counter({'NN': 2, 'DT': 1}), Counter({'NNS': 1, 'NN': 1}), Counter({'NN': 2}), Counter({'NN': 1}), Counter({'NN': 3}), Counter({'NN': 2}), Counter({'VBP': 1, 'JJ': 1, 'CD': 1, 'RB': 1, 'IN': 1, 'VB': 1}), Counter({'NN': 2, 'CC': 1}), Counter({'NN': 1}), Counter({'NN': 4, 'JJ': 1, ':': 1}), Counter({'NN': 5, 'IN': 1, 'RB': 1, 'JJS': 1, 'JJ': 1}), Counter({'NN': 4, 'RB': 1, 'VBD': 1, 'CD': 1}), Counter({'NNS': 1, 'VBP': 1}), Counter({'CD': 1, 'DT': 1, 'JJS': 1, 'NN': 1, 'RB': 1}), Counter({'NN': 4, 'JJ': 2}), Counter({'IN': 2, 'NN': 2}), Counter({'NN': 2}), Counter({'NN': 1}), Counter({'NN': 2, ':': 1}), Counter({'NN': 1}), Counter({'NN': 1, 'VBD': 1}), Counter({'NN': 2}), Counter({'NN': 9, 'VBP': 1, 'JJ': 1}), Counter({'NN': 3, 'VBP': 1, 'RB': 1}), Counter({'NN': 4, 'POS': 1}), Counter({'NN': 1}), Counter({'NN': 2, 'DT': 1}), Counter({'NN': 3, 'VBD': 1, 'JJ': 1}), Counter({'RB': 1, 'JJ': 1, 'NN': 1}), Counter({'NN': 2, 'RB': 1}), Counter({'JJ': 2, 'NN': 2, '``': 1, 'JJS': 1, 'RB': 1, \"''\": 1, 'NNS': 1}), Counter({'NN': 3, 'DT': 1, 'RB': 1, 'IN': 1}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 57, 'JJ': 19, 'VBP': 7, 'RB': 6, 'VB': 3, 'NNS': 3, 'VBN': 3, 'POS': 3, 'VBD': 2, 'DT': 2, 'IN': 2, 'JJS': 2, 'MD': 2, 'VBZ': 1, 'WRB': 1}), Counter({'NN': 26, 'JJ': 10, 'RB': 4, 'VBP': 3, 'POS': 3, 'NNS': 1, ':': 1, 'MD': 1, 'VB': 1, 'VBD': 1, 'JJS': 1, 'VBN': 1}), Counter({'NN': 18, 'JJ': 9, 'VBP': 5, 'NNS': 2, 'VBD': 1, 'PRP': 1, 'DT': 1, 'CC': 1, 'CD': 1, 'RBR': 1, 'IN': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 20, 'JJ': 8, 'JJS': 3, 'VB': 2, 'VBP': 2, 'VBZ': 1, 'DT': 1, 'CD': 1, ':': 1, 'CC': 1, 'VBG': 1, 'RB': 1}), Counter({'NN': 26, 'JJ': 10, 'VBP': 4, 'JJS': 2, 'NNS': 2, 'IN': 2, '``': 2, \"''\": 2, 'RB': 2, 'PRP': 1, 'VBN': 1, 'JJR': 1, 'VBZ': 1, 'DT': 1}), Counter({'NN': 27, 'JJ': 9, 'IN': 4, 'POS': 4, 'DT': 3, 'VBD': 3, 'VB': 2, 'VBP': 2, 'FW': 1, 'PRP': 1, 'VBZ': 1}), Counter({'NN': 27, 'JJ': 19, 'VBP': 8, 'RB': 8, 'VB': 7, 'CC': 4, 'DT': 3, 'VBD': 3, 'RBS': 2, 'NNS': 1, 'MD': 1, 'VBN': 1, 'VBZ': 1, 'IN': 1, 'PRP': 1, 'VBG': 1, 'JJS': 1}), Counter({'NN': 19, 'VB': 4, 'VBP': 3, 'JJ': 3, 'PRP': 1, 'VBZ': 1, 'FW': 1, 'MD': 1, 'POS': 1, 'CC': 1, 'RB': 1, 'DT': 1, 'IN': 1}), Counter({'NN': 77, 'JJ': 37, 'RB': 7, 'POS': 6, 'VBP': 5, 'VB': 4, 'IN': 3, 'PRP': 3, 'MD': 3, 'JJS': 3, 'VBN': 2, 'VBZ': 2, 'CC': 1, 'VBD': 1, 'DT': 1, 'NNS': 1, 'CD': 1, 'WP': 1}), Counter({'NN': 29, 'JJ': 14, 'VB': 6, 'VBZ': 3, 'VBP': 2, 'VBD': 2, 'RB': 2, 'PRP': 1, 'POS': 1, 'CC': 1, 'NNP': 1, 'TO': 1, 'RP': 1, 'JJS': 1}), Counter({'NN': 20, 'JJ': 13, 'RB': 9, 'VB': 7, 'VBP': 5, 'MD': 2, 'NNS': 1, 'VBD': 1, 'CD': 1, '``': 1, \"''\": 1}), Counter({'NN': 31, 'JJ': 14, 'VBP': 6, 'VBD': 5, 'RB': 5, 'NNS': 3, 'VB': 3, 'PRP': 2, 'IN': 2, 'DT': 1, '``': 1, \"''\": 1, 'VBZ': 1, 'MD': 1, 'JJS': 1}), Counter({'NN': 10, 'JJ': 2, 'VBP': 2, 'DT': 1, 'NNS': 1}), Counter({'NN': 7, 'JJ': 2, 'VBP': 1}), Counter({'NN': 4, 'JJ': 3, 'DT': 1, 'VBP': 1, 'RB': 1}), Counter({'NN': 65, 'JJ': 32, 'VBD': 9, 'VB': 6, 'VBP': 5, 'IN': 3, 'RB': 3, 'MD': 3, 'DT': 2, ':': 1, 'WDT': 1, 'WP': 1, 'TO': 1, 'NNS': 1, 'CC': 1}), Counter({'NN': 34, 'JJ': 10, 'VBP': 4, 'NNS': 2, 'RB': 2, 'POS': 2, 'VB': 1, 'PRP': 1, 'IN': 1}), Counter({'NN': 89, 'JJ': 39, 'VBP': 12, 'VB': 10, 'RB': 8, 'POS': 7, 'NNS': 4, 'VBZ': 3, 'IN': 3, 'CD': 3, 'MD': 2, 'JJS': 2, 'VBD': 2, 'PRP': 1, 'VBN': 1, 'VBG': 1}), Counter({'NN': 81, 'JJ': 36, 'RB': 16, 'VBD': 14, 'VB': 12, 'VBP': 9, 'VBN': 4, 'MD': 4, 'POS': 4, 'PRP': 4, 'VBZ': 4, 'DT': 3, 'IN': 3, 'CD': 2, ':': 1, 'EX': 1, 'WP': 1, 'NNS': 1, 'CC': 1}), Counter({'NN': 38, 'JJ': 18, 'VB': 4, 'RB': 4, 'VBP': 4, 'VBD': 3, 'CC': 2, 'IN': 2, 'NNS': 1, 'MD': 1, 'DT': 1, 'RBR': 1}), Counter({'NN': 40, 'JJ': 15, 'VBP': 7, 'VBN': 4, 'VBD': 3, 'IN': 3, 'DT': 3, 'VBZ': 2, 'NNS': 2, 'POS': 2, 'RB': 2, 'FW': 1, 'JJS': 1, 'VB': 1, 'CD': 1, 'RBR': 1, 'PRP': 1, 'MD': 1}), Counter({'NN': 32, 'JJ': 8, 'VBP': 6, 'IN': 5, 'VB': 4, 'RB': 3, 'CD': 3, 'VBD': 2, 'DT': 1, 'MD': 1, 'POS': 1}), Counter({'NN': 27, 'JJ': 11, 'VBP': 8, ':': 3, 'IN': 3, 'VBD': 2, 'VBN': 2, 'CC': 1, 'JJS': 1, 'POS': 1, 'RBR': 1, 'RB': 1, 'MD': 1, 'VB': 1}), Counter({'NN': 33, 'VBP': 5, 'JJ': 5, 'VB': 4, 'RB': 4, 'NNS': 4, 'VBD': 4, 'CD': 3, 'PRP$': 1, 'TO': 1, 'JJS': 1, 'DT': 1, 'PRP': 1}), Counter({'NN': 32, 'JJ': 13, 'DT': 5, 'VB': 3, 'VBP': 3, 'NNS': 2, 'RB': 2, 'VBD': 2, 'IN': 2, 'MD': 2, 'POS': 1}), Counter({'NN': 32, 'JJ': 13, 'DT': 5, 'VB': 3, 'VBP': 3, 'NNS': 2, 'RB': 2, 'VBD': 2, 'IN': 2, 'MD': 2, 'POS': 1}), Counter({'NN': 52, 'JJ': 21, 'VBP': 15, 'VB': 8, 'IN': 5, 'PRP': 4, 'RB': 4, 'VBZ': 3, 'POS': 2, 'DT': 2, 'NNS': 2, 'VBN': 2, 'JJR': 2, 'MD': 2, 'VBD': 1, 'JJS': 1, 'EX': 1, 'WDT': 1, '``': 1, \"''\": 1, 'CD': 1}), Counter({'NN': 30, 'JJ': 17, 'VBD': 5, 'RB': 5, 'VBP': 4, 'DT': 4, 'NNS': 3, 'POS': 3, 'IN': 3, 'WRB': 2, 'VB': 1, 'VBZ': 1, 'PRP': 1}), Counter({'NN': 135, 'JJ': 47, 'IN': 11, 'RB': 10, 'VBP': 10, 'DT': 9, 'VB': 8, 'VBD': 8, 'MD': 5, 'VBZ': 3, 'JJR': 2, 'PRP': 2, 'VBG': 2, 'POS': 2, 'RBR': 2, 'NNS': 1, 'FW': 1, ':': 1, 'CD': 1, 'WP$': 1}), Counter({'NN': 30, 'DT': 5, 'JJ': 4, \"''\": 1, 'CD': 1, 'VBG': 1, 'PRP': 1, 'VBP': 1}), Counter({'NN': 27, 'JJ': 8, 'RB': 8, 'IN': 5, 'VB': 5, 'VBP': 4, 'NNS': 2, 'DT': 2, 'MD': 2, '``': 1, \"''\": 1, 'VBN': 1, 'FW': 1, 'POS': 1, 'VBD': 1, 'CC': 1, 'RBR': 1, 'VBZ': 1}), Counter({'NN': 27, 'JJ': 10, 'VBD': 6, 'DT': 2, 'VBP': 2, 'RB': 2, 'IN': 1, 'FW': 1, 'CC': 1, 'CD': 1, 'PRP$': 1, 'JJS': 1}), Counter({'NN': 15, 'VBP': 4, 'JJ': 4, 'IN': 3, 'VB': 2, 'DT': 1, 'PRP': 1, 'NNS': 1, 'RB': 1, '``': 1, \"''\": 1, 'VBZ': 1, 'JJR': 1, 'POS': 1}), Counter({'NN': 57, 'JJ': 29, 'VBP': 13, 'RB': 6, 'VBN': 5, 'VB': 3, 'VBD': 3, 'IN': 3, 'MD': 2, 'JJR': 2, 'CC': 1, 'DT': 1, 'JJS': 1, ':': 1, 'PRP': 1, 'NNS': 1}), Counter({'NN': 116, 'JJ': 58, 'VBP': 27, 'RB': 19, 'IN': 16, 'VB': 14, '``': 7, 'VBD': 6, 'NNS': 6, 'MD': 4, 'VBN': 4, \"''\": 3, 'POS': 3, 'VBZ': 3, 'RP': 2, 'DT': 1, 'CD': 1, 'JJR': 1, 'PRP$': 1, 'RBR': 1, 'JJS': 1, 'PRP': 1}), Counter({'NN': 60, 'JJ': 17, 'DT': 7, 'VB': 7, 'CD': 6, 'RB': 4, 'VBP': 4, 'POS': 3, 'JJS': 3, 'IN': 3, 'MD': 3, 'JJR': 3, 'VBD': 2, 'VBN': 1, 'VBZ': 1, 'NNP': 1, 'NNS': 1, 'RP': 1}), Counter({'NN': 198, 'JJ': 74, 'VBP': 24, 'RB': 22, 'VB': 19, 'IN': 12, 'POS': 10, 'VBD': 7, '``': 6, \"''\": 6, 'CD': 5, 'DT': 5, 'CC': 5, 'VBN': 4, 'MD': 4, 'JJS': 2, 'VBG': 2, 'NNS': 2, 'FW': 2, 'TO': 2, 'VBZ': 1, 'NNP': 1, 'PRP': 1, ':': 1}), Counter({'NN': 10, 'JJS': 4, 'JJ': 3, 'VBP': 3, 'RB': 2, 'VB': 2, 'NNS': 1, 'JJR': 1, 'IN': 1, 'CD': 1}), Counter({'NN': 68, 'JJ': 24, 'VBP': 11, 'RB': 8, 'IN': 5, 'POS': 5, 'VB': 5, 'MD': 3, 'VBN': 2, 'NNS': 2, 'WP$': 1, 'RBR': 1, 'VBZ': 1}), Counter({'NN': 44, 'JJ': 20, 'VB': 10, 'VBP': 6, 'MD': 5, 'NNS': 3, 'RB': 3, 'IN': 3, 'CD': 3, 'PRP': 2, 'VBD': 1, 'CC': 1, 'DT': 1, '``': 1, \"''\": 1, 'POS': 1, 'FW': 1, 'EX': 1, 'VBG': 1}), Counter({'NN': 80, 'JJ': 28, 'VBP': 10, 'VB': 7, 'RB': 6, 'NNS': 4, 'IN': 4, '``': 3, 'CD': 3, 'POS': 3, 'MD': 3, 'CC': 2, 'VBZ': 2, 'VBD': 2, \"''\": 1, 'JJR': 1, 'DT': 1, 'VBN': 1, 'PRP': 1, 'JJS': 1, ':': 1, 'WP': 1, 'EX': 1}), Counter({'NN': 9, 'CD': 2, 'JJS': 2, 'VBZ': 1, 'VBP': 1, 'RB': 1}), Counter({'NN': 97, 'JJ': 36, 'RB': 16, 'VBP': 14, 'IN': 11, 'VB': 10, 'VBD': 7, 'PRP': 6, 'DT': 5, 'MD': 4, 'VBN': 4, 'CC': 3, 'CD': 3, 'RP': 3, 'VBZ': 2, 'JJS': 2, 'POS': 2, 'JJR': 1, 'RBR': 1, 'NNS': 1}), Counter({'NN': 21, 'JJ': 6, 'VB': 3, 'POS': 2, 'IN': 2, 'VBP': 2, 'VBD': 1, 'MD': 1, 'NNS': 1}), Counter({'NN': 12, 'JJ': 5, 'RB': 5, 'VBP': 4, 'VBD': 2, 'VB': 2, 'IN': 2, 'NNS': 1, 'DT': 1, 'VBN': 1, ':': 1}), Counter({'NN': 78, 'JJ': 25, 'RB': 7, 'VB': 7, 'VBP': 7, 'VBD': 5, 'IN': 3, 'PRP': 2, 'VBZ': 2, 'MD': 2, 'DT': 2, 'VBN': 2, 'FW': 1, 'JJR': 1}), Counter({'NN': 23, 'JJ': 6, 'VBD': 4, 'RB': 3, 'IN': 3, 'WP': 1, 'VB': 1, 'FW': 1, 'CC': 1}), Counter({'NN': 17, 'JJ': 10, 'VBP': 2, 'VBD': 2, 'RB': 2, 'VBZ': 2, 'POS': 1, 'IN': 1, 'NNS': 1, 'DT': 1, 'NNP': 1, 'RBR': 1, ':': 1, 'PRP': 1, 'VB': 1}), Counter({'NN': 16, 'JJ': 9, 'VB': 5, 'VBP': 3, 'RB': 3, 'VBD': 2, 'IN': 2, 'NNS': 1, 'MD': 1, 'CD': 1}), Counter({'NN': 17, 'JJ': 6, 'VBP': 4, 'VBD': 3, 'CD': 3, 'VB': 3, 'PRP': 2, 'RP': 1, 'JJS': 1, 'VBN': 1, 'RB': 1, 'CC': 1}), Counter({'NN': 177, 'JJ': 78, 'VB': 31, 'VBP': 26, 'RB': 22, 'MD': 16, 'IN': 15, 'PRP': 13, 'VBD': 11, 'CC': 9, 'POS': 9, 'CD': 6, 'VBZ': 5, 'DT': 5, 'NNS': 4, 'JJR': 2, 'RBR': 1, 'EX': 1, 'WDT': 1, 'VBN': 1, 'FW': 1}), Counter({'NN': 19, 'JJ': 7, ':': 3, 'DT': 2, 'VBP': 2, 'POS': 1, 'CC': 1, 'IN': 1, 'VB': 1, 'JJS': 1, 'WP': 1}), Counter({'NN': 19, 'JJ': 13, 'RB': 4, 'VBD': 3, 'VBP': 2, 'VB': 2, 'NNS': 1, 'PRP': 1, 'CC': 1, 'VBN': 1}), Counter({'NN': 10, 'JJ': 5, 'RB': 2, 'VBP': 2, 'VBD': 1}), Counter({'NN': 33, 'JJ': 16, 'RB': 6, 'VB': 6, 'VBP': 4, 'IN': 4, 'NNS': 3, 'VBD': 3, 'POS': 2, '``': 2, \"''\": 2, 'CD': 2, 'DT': 2, 'MD': 2, 'FW': 1, 'WP$': 1, 'VBZ': 1}), Counter({'NN': 11, 'VBP': 2, 'JJ': 2, 'DT': 1, 'POS': 1, 'VBN': 1}), Counter({'NN': 26, 'VBP': 6, 'JJ': 5, 'VB': 2, 'POS': 2, 'VBZ': 1, 'RB': 1}), Counter({'NN': 25, 'JJ': 11, 'VBP': 5, 'RB': 4, 'IN': 3, 'VBD': 3, 'CD': 2, 'VB': 2, 'POS': 1, 'VBN': 1, 'NNS': 1, 'MD': 1}), Counter({'NN': 139, 'JJ': 42, 'VBP': 19, 'RB': 17, 'VB': 12, 'VBD': 10, 'IN': 9, 'CD': 8, 'POS': 5, 'VBZ': 5, 'MD': 4, 'PRP': 3, 'VBN': 3, 'RBR': 2, 'NNS': 2, 'DT': 2, 'FW': 1, 'JJR': 1, 'EX': 1, 'RBS': 1, 'JJS': 1, 'PDT': 1, 'NNP': 1}), Counter({'NN': 21, 'JJ': 10, 'VB': 5, 'POS': 5, 'VBP': 3, 'PRP': 2, 'VBZ': 2, 'RB': 2, 'IN': 2, 'CD': 1, 'VBD': 1, 'JJR': 1, 'WRB': 1, 'CC': 1}), Counter({'NN': 14, 'RB': 4, 'VB': 4, 'VBP': 3, 'JJ': 3, ':': 2, 'VBD': 2, 'JJS': 1, 'VBN': 1, 'MD': 1, 'IN': 1, 'CC': 1, 'CD': 1}), Counter({'NN': 16, 'JJ': 5, 'IN': 3, 'VBP': 2, 'VB': 2, 'PRP': 1, 'VBZ': 1, 'CC': 1, 'RB': 1, 'POS': 1, 'VBN': 1, 'TO': 1}), Counter({'NN': 25, 'VBP': 4, 'VB': 3, 'JJ': 2, 'VBZ': 2, 'IN': 1, 'RB': 1, 'VBD': 1, 'WDT': 1}), Counter({'NN': 126, 'JJ': 59, 'RB': 30, 'VBP': 20, 'VB': 16, 'IN': 15, 'POS': 11, 'VBD': 10, 'VBZ': 7, 'PRP': 6, 'CD': 6, 'DT': 4, 'NNS': 4, 'MD': 3, 'JJR': 2, 'JJS': 2, 'FW': 1, 'CC': 1, 'VBN': 1, 'TO': 1, 'WP': 1, '``': 1, \"''\": 1}), Counter({'NN': 1}), Counter({'NN': 50, 'JJ': 24, '``': 8, \"''\": 8, 'POS': 6, 'VBP': 6, ':': 4, 'PRP': 3, 'IN': 3, 'DT': 3, 'RB': 3, 'VBD': 2, 'VBZ': 2, 'NNS': 2, 'VB': 2, 'VBN': 1, 'CC': 1, 'WDT': 1, 'MD': 1}), Counter({'NN': 41, 'JJ': 7, 'IN': 5, 'VBP': 3, 'RB': 2, 'POS': 2, 'CC': 2, 'VBD': 2, 'VBN': 2, 'JJS': 1, 'PRP': 1, 'VBZ': 1, 'DT': 1, 'NNS': 1, 'TO': 1, 'VB': 1, 'WRB': 1, 'RBR': 1}), Counter({'NN': 13, 'JJ': 5, 'IN': 4, 'RB': 2, 'DT': 2, 'VBP': 1, 'VBN': 1, 'FW': 1, 'VB': 1, 'VBZ': 1}), Counter({'NN': 18, 'JJ': 5, 'VBP': 3, 'RB': 2, 'VB': 2, 'IN': 1, 'PRP': 1, 'VBZ': 1, 'NNS': 1, 'DT': 1, 'VBN': 1, 'CD': 1}), Counter({'NN': 94, 'JJ': 30, 'VBP': 15, 'RB': 14, 'IN': 9, 'DT': 7, 'VBD': 6, 'NNS': 6, 'VBZ': 5, 'POS': 4, 'VB': 4, 'FW': 3, 'PRP': 2, 'EX': 1, 'JJR': 1, 'CC': 1}), Counter({'NN': 9, 'JJ': 5, 'VBD': 3, 'MD': 2, 'VB': 2, ':': 2, 'VBN': 2, 'DT': 1, 'PRP': 1, 'RB': 1, 'VBP': 1}), Counter({'NN': 44, 'JJ': 16, 'VBP': 13, 'RB': 6, 'VB': 3, 'DT': 3, 'IN': 3, 'VBN': 2, 'VBD': 2, 'VBZ': 2, 'PRP': 1, 'NNS': 1, 'POS': 1, 'EX': 1, 'RBR': 1}), Counter({'NN': 18, 'JJ': 7, 'VB': 4, 'RB': 2, 'CC': 2, 'PRP': 2, 'POS': 2, 'VBD': 1, 'VBP': 1, 'DT': 1, 'VBZ': 1, 'IN': 1, 'WP': 1}), Counter({'NN': 37, 'JJ': 22, 'VBP': 6, 'IN': 5, 'VB': 4, 'DT': 3, 'CD': 3, 'NNS': 2, 'VBN': 2, 'POS': 2, 'CC': 2, 'RB': 2, 'MD': 1, 'JJS': 1}), Counter({'NN': 8, 'JJS': 2, 'CD': 1, 'VBN': 1, 'POS': 1, 'JJ': 1}), Counter({'NN': 96, 'JJ': 35, 'VBP': 10, 'VB': 8, 'RB': 7, 'IN': 6, 'VBZ': 4, 'VBD': 4, 'PRP': 4, 'MD': 4, 'NNS': 4, ':': 2, 'POS': 2, 'JJS': 2, 'VBN': 2, 'WDT': 2, 'RBR': 1, 'JJR': 1, 'DT': 1, 'CC': 1, '``': 1, \"''\": 1, 'CD': 1}), Counter({'NN': 26, 'JJ': 5, 'VBP': 3, 'VBD': 3, 'IN': 3, 'VB': 2, 'DT': 2, 'CD': 1, 'NNS': 1, 'VBN': 1, 'PRP': 1, 'RB': 1, 'NNP': 1}), Counter({'NN': 29, 'JJ': 4, 'VBP': 4, 'VBD': 3, 'IN': 2, 'RB': 2, 'PRP': 1, 'VBZ': 1, 'VBN': 1, 'NNS': 1, 'MD': 1, 'VB': 1, 'DT': 1, 'POS': 1}), Counter({'NN': 13, 'DT': 2, 'VBZ': 2, 'JJ': 2, 'RB': 1, 'VBN': 1, 'VBD': 1, 'RBR': 1, 'IN': 1}), Counter({'NN': 4, 'JJS': 1, 'JJ': 1, 'VBP': 1}), Counter({'NN': 82, 'JJ': 36, 'VBP': 11, 'RB': 8, 'VBD': 7, 'IN': 6, 'POS': 5, 'VBN': 3, 'DT': 3, 'VB': 3, 'CD': 2, 'JJS': 2, 'MD': 2, 'NNS': 2, 'PRP': 1, 'CC': 1}), Counter({'NN': 21, 'JJ': 4, 'RBR': 2, 'DT': 1, 'TO': 1, 'VB': 1, '``': 1, 'CD': 1, 'JJS': 1, \"''\": 1}), Counter({'NN': 14, 'JJ': 3, 'WP': 1, 'VBZ': 1, 'VBP': 1, 'VB': 1, 'PRP': 1, 'VBD': 1, 'RB': 1, 'VBN': 1, 'IN': 1, 'DT': 1}), Counter({'NN': 27, 'JJ': 6, 'VBP': 3, 'NNS': 2, 'VBD': 2, 'VBN': 1, 'PRP': 1, 'RB': 1, 'VB': 1, 'IN': 1}), Counter({'NN': 28, 'JJ': 7, '``': 5, \"''\": 5, 'RB': 2, 'IN': 2, 'RBR': 2, 'VBP': 1, 'VB': 1, 'VBD': 1, 'VBN': 1}), Counter({'NN': 7, 'VBP': 5, 'RB': 2, 'JJ': 2, 'NNS': 1, 'DT': 1}), Counter({'NN': 40, 'JJ': 29, 'VB': 7, 'MD': 6, 'RB': 6, 'VBP': 5, 'DT': 3, 'IN': 3, 'POS': 3, 'VBZ': 2, 'VBN': 2, 'WP': 1, 'WDT': 1, 'JJR': 1, 'CD': 1, 'NNS': 1}), Counter({'NN': 14, 'FW': 1, '$': 1, 'CD': 1, 'JJ': 1, 'VBZ': 1, 'JJR': 1}), Counter({'NN': 14, 'JJ': 3, 'POS': 1, ':': 1, 'VBD': 1, 'RB': 1, 'VBN': 1, 'VBZ': 1, 'DT': 1, 'VB': 1, 'IN': 1}), Counter({'NN': 9, 'JJ': 6, 'VBD': 1, 'VB': 1}), Counter({'NN': 32, 'JJ': 16, 'DT': 6, 'POS': 3, 'VBZ': 3, 'VBD': 3, 'RB': 3, 'VBN': 2, 'IN': 2, 'EX': 1, 'PRP': 1, 'JJR': 1, 'VBP': 1, 'VB': 1}), Counter({'NN': 55, 'JJ': 17, 'VB': 8, 'IN': 7, 'RB': 6, 'VBP': 5, ':': 5, 'PRP': 4, 'VBD': 3, 'VBZ': 3, 'WP': 2, 'JJS': 2, 'CC': 2, 'FW': 1, 'VBN': 1, '``': 1, \"''\": 1, 'POS': 1, 'MD': 1, 'DT': 1}), Counter({'NN': 26, 'RB': 11, 'JJ': 11, 'VBP': 7, 'VB': 7, 'VBD': 2, 'MD': 2, 'WRB': 1, 'CD': 1, 'POS': 1, 'PRP': 1, 'VBN': 1, 'NNS': 1}), Counter({'NN': 103, 'JJ': 43, 'VBP': 20, 'VB': 12, 'IN': 9, 'VBD': 8, 'DT': 8, 'RB': 7, 'NNS': 6, 'PRP': 5, 'VBN': 4, 'VBZ': 4, 'CC': 4, 'CD': 3, 'MD': 2, 'JJS': 2, 'VBG': 1, '``': 1, \"''\": 1, 'WRB': 1, 'FW': 1}), Counter({'NN': 57, 'JJ': 22, 'RB': 9, 'VBZ': 6, '``': 4, \"''\": 4, 'VBN': 3, 'POS': 3, 'VBP': 3, 'VB': 3, 'VBD': 2, 'DT': 2, ':': 2, 'JJS': 1, 'IN': 1, 'NNS': 1, 'VBG': 1, 'MD': 1, 'PRP': 1, 'FW': 1, 'NNP': 1}), Counter({'NN': 47, 'VBP': 11, 'VB': 11, 'JJ': 11, 'RB': 9, 'MD': 6, 'PRP': 4, 'IN': 4, 'NNS': 3, 'VBD': 3, 'VBZ': 3, 'JJS': 3, 'CD': 1, 'POS': 1, 'CC': 1}), Counter({'NN': 10, 'JJ': 2, 'DT': 1, 'CD': 1, 'POS': 1}), Counter({'NN': 15, 'JJ': 9, 'IN': 3, 'RB': 2, 'DT': 2, 'PRP': 1, 'VBN': 1, 'VBD': 1, 'NNS': 1, 'VBP': 1}), Counter({'NN': 6, 'JJ': 2, 'DT': 1, 'VB': 1, 'WRB': 1}), Counter({'NN': 17, 'JJ': 4, 'VBD': 2, 'VBP': 2, 'RB': 2, 'VB': 1}), Counter({'NN': 3, 'VB': 2, 'VBP': 1, 'JJ': 1}), Counter({'NN': 5, 'JJ': 3, 'VBP': 2, 'RB': 1, 'VB': 1, 'POS': 1, '``': 1, \"''\": 1, 'VBD': 1}), Counter({'NN': 16, 'JJ': 8, 'JJS': 4, 'RBS': 3, 'MD': 2, 'VB': 2, 'RB': 1, 'VBD': 1, 'NNS': 1, 'VBP': 1, 'IN': 1}), Counter({'NN': 30, 'VB': 10, 'JJ': 9, 'RB': 7, 'MD': 5, 'VBP': 4, 'DT': 2, 'IN': 2, 'POS': 2, 'TO': 1, 'NNS': 1, 'FW': 1, ':': 1, 'WRB': 1}), Counter({'NN': 99, 'JJ': 28, 'VBP': 6, 'RB': 6, 'IN': 6, 'DT': 4, 'VB': 4, 'VBD': 4, 'PRP': 4, 'VBZ': 4, 'CD': 2, ':': 2, 'MD': 1, 'VBN': 1, 'WP': 1, 'CC': 1, 'NNS': 1}), Counter({'NN': 37, 'JJ': 12, 'VBP': 5, 'VB': 5, 'POS': 4, 'VBD': 3, '``': 3, 'DT': 3, 'IN': 2, 'RB': 2, 'CC': 2, 'NNS': 1, 'WRB': 1, \"''\": 1, 'MD': 1, 'VBZ': 1, 'VBN': 1, 'CD': 1}), Counter({'NN': 78, 'JJ': 34, 'VBP': 6, 'RB': 6, 'FW': 4, 'IN': 4, 'VBD': 4, 'VB': 4, 'NNS': 3, 'POS': 2, 'VBN': 2, 'WP$': 2, '``': 1, \"''\": 1, 'RBR': 1, 'MD': 1}), Counter({'NN': 113, 'JJ': 29, 'VBP': 20, 'VB': 12, ':': 12, 'RB': 8, 'IN': 7, 'CC': 7, '``': 6, 'POS': 5, 'VBD': 5, 'DT': 4, 'VBZ': 4, 'VBN': 3, 'PRP': 3, 'NNS': 2, 'CD': 2, 'WP': 2, \"''\": 2, 'MD': 1, 'RP': 1, 'JJR': 1}), Counter({'NN': 50, 'JJ': 20, 'POS': 6, 'VBZ': 4, 'VBP': 4, 'PRP': 2, 'IN': 2, 'VBD': 2, 'WDT': 1, 'RB': 1, 'FW': 1, 'NNS': 1, 'CD': 1, 'JJS': 1, 'CC': 1}), Counter({'NN': 26, 'JJ': 14, 'VBD': 4, 'RB': 4, 'VBP': 2, 'PRP': 2, 'POS': 1, 'MD': 1, 'VB': 1, 'WP$': 1, 'IN': 1, '``': 1, \"''\": 1, 'CD': 1}), Counter({'NN': 16, 'JJ': 6, 'VBP': 3, 'VB': 3, 'RB': 2, 'VBN': 1, 'DT': 1, 'VBD': 1, 'MD': 1}), Counter({'NN': 41, 'JJ': 22, 'RB': 11, 'VBP': 7, 'POS': 6, 'VBZ': 5, 'CC': 4, 'DT': 4, 'VBD': 3, 'NNS': 3, 'WP': 2, 'PRP': 2, 'IN': 2, 'CD': 1, 'MD': 1, 'VB': 1, ':': 1, '``': 1, \"''\": 1, 'VBN': 1}), Counter({'NN': 37, 'JJ': 8, 'VBP': 5, 'RB': 4, 'DT': 3, 'VBD': 3, 'POS': 3, 'VB': 3, 'PRP': 1, 'JJR': 1, 'IN': 1, '``': 1, \"''\": 1, 'VBZ': 1, 'NNS': 1, 'MD': 1}), Counter({'NN': 30, 'JJ': 13, 'DT': 3, 'RB': 3, 'IN': 3, 'VBP': 3, 'VBD': 2, 'VBN': 1, 'CD': 1, 'MD': 1, 'VB': 1}), Counter({'NN': 10, 'DT': 1, 'JJ': 1, 'IN': 1})]\n",
            "The total verbs: 113\n",
            "The total nouns: 222\n",
            "The total adjectives: 157\n",
            "The total adverbs: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    }
  ]
}