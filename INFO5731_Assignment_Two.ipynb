{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/Huyen_INFO5731_Spring2020/blob/master/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPjGGLvpVGE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzMjKmiPTHlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Question1\n",
        "\n",
        "#Yesss I fixed that problem\n",
        "\n",
        "host_url = [\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"]\n",
        "load_more_urls = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "all_urls = host_url+load_more_urls\n",
        "user_names, user_links, content, date, title, review_helpfulness = [],[], [], [], [], []\n",
        "review_elements = []\n",
        "for url in all_urls:\n",
        "  request = urllib.request.urlopen (url)\n",
        "  webpage  = request.read()\n",
        "  soup = BeautifulSoup(webpage)\n",
        "\n",
        "  reviews = soup.find_all(\"div\", {\"class\":\"lister-item-content\" })\n",
        "  for review in reviews:\n",
        "    try:\n",
        "        review_title = review.find(\"a\", class_=\"title\").text.strip()\n",
        "    except:\n",
        "        review_title = \"\"\n",
        "    try:\n",
        "        review_id = review.find(\"a\")[\"href\"].split(\"/\")[2]\n",
        "    except:\n",
        "        review_id = \"\"\n",
        "    try:\n",
        "        review_date = review.find(\"span\", class_=\"review-date\").get_text()\n",
        "    except:\n",
        "        review_date = \"\"\n",
        "    try:\n",
        "        review_nickname = review.find (\"span\", class_=\"display-name-link\").get_text()\n",
        "    except:\n",
        "        review_nickname = \"\"\n",
        "    try:\n",
        "        review_content = review.find (\"div\", class_=\"text show-more__control\").get_text()\n",
        "    except:\n",
        "        review_content =\"\"\n",
        "    #print(review_content)\n",
        "    try:\n",
        "        review_rating = review. find (\"span\", class_=\"rating-other-user-rating\").getText().strip()\n",
        "        #print(review_rating)\n",
        "    except:\n",
        "        review_rating = \"\"\n",
        "    try:\n",
        "        review_helpfulness = review.find(\"div\", {\"class\": \"actions text-muted\"}).text.split(\"\\n\")[1]\n",
        "        #print(review_helpfulness)\n",
        "    except:\n",
        "        review_helpfulness = \"\"\n",
        "\n",
        "    review_elements.append([review_title, review_id, review_date, review_nickname, review_content, review_rating, review_helpfulness])\n",
        "#print(len(review_elements))\n",
        "\n",
        "df = pd.DataFrame(review_elements, columns=[\"review_title\", \"review_id\", \"review_date\", \"review_nickname\", \"review_content\", \"review_rating\", \"review_helpfulness\"])\n",
        "df\n",
        "\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"w\", newline=\"\", encoding='utf-8') as file:\n",
        "  df.to_csv(file)\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"r\", encoding='utf-8') as file:\n",
        "  pd.read_csv(file)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6v-2mpvil47",
        "colab_type": "code",
        "outputId": "1ebdf538-938f-4b05-bc95-4380ead40255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#try_codes\n",
        "load_more_data = soup.find_all(\"div\", attrs={\"class\": \"load-more-data\"}) # find this button at the end of each page\n",
        "#print(load_more_data)\n",
        "for item in load_more_data:\n",
        "  path_parse1 = \"https://www.imdb.com\"\n",
        "  path_parse2 = \"/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=\"\n",
        "  path_parse3 = item.get(\"data-key\")\n",
        "  concatenated_path = path_parse1+path_parse2+path_parse3\n",
        "  print(concatenated_path)\n",
        "#load_more_links = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "#for link in load_more_links:\n",
        " # print(link) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqu2ta3qb7oux7nrvqpr42brhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k4rmwb65sqjd2c5p4ic5kj7njmla\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBytY15j3Hkx",
        "colab_type": "code",
        "outputId": "282e83d0-3f7a-48b4-c4eb-2537e8985b4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "# In this line, I am attempting to generate links with data-key in the class load-more-data. However, because, the server prohibits to get key in the second next page, so my code raise error.\n",
        "host_url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "def link_load_more (host_url):\n",
        "  request = urllib.request.urlopen (host_url)\n",
        "  webpage  = request.read()\n",
        "  soup = BeautifulSoup(webpage)\n",
        "  load_more_data = soup.find_all(\"div\", attrs={\"class\": \"load-more-data\"}) # find this button at the end of each page\n",
        "  flag = True\n",
        "  if len (load_more_data):\n",
        "  #print(load_more_data)\n",
        "    for item in load_more_data:\n",
        "      path_parse1 = \"https://www.imdb.com/\"\n",
        "      path_parse2 = \"title/tt7286456/reviews/_ajax\"\n",
        "      path_parse3 = \"?ref_=undefined&paginationKey=\"\n",
        "      try:\n",
        "        path_parse4 = item.get(\"data-key\")\n",
        "        concatenated_path = path_parse1+path_parse2+path_parse3+path_parse4\n",
        "      except KeyError:\n",
        "        flag = False\n",
        "  return concatenated_path\n",
        "\n",
        "# generate link and parse all of four pages\n",
        "\n",
        "url_nextpage1 = link_load_more(host_url)\n",
        "url_nextpage2 = link_load_more(url_nextpage1)\n",
        "url_nextpage3 = link_load_more(url_nextpage2)\n",
        "url_nextpage4 = link_load_more(url_nextpage3)\n",
        "\n",
        "print(url_nextpage1)\n",
        "print(url_nextpage2)\n",
        "print(url_nextpage3)\n",
        "print(url_nextpage4)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-79216aae5d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# generate link and parse all of four pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0murl_nextpage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0murl_nextpage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_nextpage1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0murl_nextpage3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_nextpage2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-79216aae5d21>\u001b[0m in \u001b[0;36mlink_load_more\u001b[0;34m(host_url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhost_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlink_load_more\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhost_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhost_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mwebpage\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'urllib' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q15UiDVAYmr8",
        "colab_type": "code",
        "outputId": "2b6d7b3a-c464-49a3-9e1a-7c16f22b4305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "link = 'https://www.imdb.com/title/tt7286456/reviews'\n",
        "r=requests.get(link)\n",
        "r.request.headers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-Agent': 'python-requests/2.21.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD9-9ZQnPi2F",
        "colab_type": "code",
        "outputId": "c9f39ad7-9e31-4686-ad2e-9251137768df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        }
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "link = 'https://www.imdb.com/title/tt7286456/reviews'\n",
        "UA = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "    'Accept-Language': 'en',\n",
        "    'Accept-Encoding': 'gzip, deflate',\n",
        "    'Connection': 'close',\n",
        "    'Upgrade-Insecure-Requests': '1'\n",
        "}\n",
        "\n",
        "getlink = session.get(link, headers=UA)\n",
        "resplink = BeautifulSoup(getlink.content, 'html.parser')\n",
        "key = resplink.find('div', {'class':'load-more-data'})\n",
        "datakey = key.attrs['data-key']\n",
        "\n",
        "UAloadmore = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0',\n",
        "    'Accept': '*/*',\n",
        "    'Accept-Language': 'en',\n",
        "    'Accept-Encoding': 'gzip, deflate',\n",
        "    'X-Requested-With': 'XMLHttpRequest',\n",
        "    'Connection': 'close',\n",
        "    'Referer': 'https://www.imdb.com/title/tt7286456/reviews'\n",
        "}\n",
        "\n",
        "a = True\n",
        "while a:\n",
        "    if key is None:\n",
        "        print('Loi')\n",
        "        a = False\n",
        "    else:\n",
        "        linkloadmore = 'https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey='+datakey\n",
        "        getloadmore = session.get(linkloadmore, headers=UAloadmore)\n",
        "        resploadmore = BeautifulSoup(getloadmore.content, 'html.parser')\n",
        "        key = resploadmore.find('div', {'class':'load-more-data'})\n",
        "        datakey = key.attrs['data-key']\n",
        "        print('https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey='+datakey)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmq45deyqd72xh5nrrrhrm4azhzfmxvlnomwklyczuf43o6ss6oe4f3njmdz4k4da6ioyalzd4xa252qu5ftukbqy\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkry3dkzaf62vhtojxrxs4oab5y4hhzo5ziwr26fbyhvrl4ty4ouyflpzidvwndtuwrturx3vuipk6vekqxnoducax\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbiq45dozib7cwxfnrqrlumsbbhzfmxvlnomwklyczuf43o6ss6oizvjmbpdr4k5bqann64snddzfb2neeojfggo6i\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cboqe3t6yac7stx5obsrhrmqabyy4hhzo5ziwr26fbyhvrl4ty4ouyftmrpdjwndtvdmtgkb2h76sxkaxt4g3rbseru\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbnry5di3yh7kwxhmrrrdtmwhjjtzpwzouokkd2gbzgpnt6ucc2pa4vxnjib4d7boidprjzxgrueniiccsyawgw6\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbmqa3d4yqd7ktx3njzrts4obrhzfmxvlnomwklyczuf43o6ss6oizfzpzicv4k4situm4cwdnyvp6bafuubwci3ri\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7dzfqa3d63qh66vh5obvr3um2abhzfmxvlnomwklyczuf43o6ss6oe4vlnridz4k46vej4jc4olmdtctbzqkfpwpmjy\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7dzeq45tozyc7ctx3mbvr7unee36tbexxgvzigmk6flsfjrkqdc5ou3vtpryoblaj7bq5m2znngnpjexxabr64fq\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7dzlqm3da3ql7kth3njuqlum6abhzfmxvlnomwklyczuf43o6ss6oe3f7nrldv4k4zq2vdqnn6yna5xrvgxgy2hhk7q\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7dzlqy2dazae7guhvmrqrlsmwbjyy4hhzo5ziwr26fbyhvrl4ty4ouyfvnbjcvx5dtsap27yajsmm6qpn4lxjvxmryy4\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7dzkqeytcyag7sxhtnjuqpu4sbr6y4hhzo5ziwr26fbyhvrl4ty4ouzvvnrjcvx5dtx5u4xvwob72lmt5cmaucz7xnfu\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7dzkqi3d4yqg66xxvnzxrptmsaj6y4hhzo5ziwr26fbyhvrl4ty4ouzvznjcdzvndtsdwuvkcxwkjyyopv5mig2v3ijg\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7dzkq45t4zal72xxxmzrrdrmybbyy4hhzo5ziwr26fbyhvrl4ty4ouyfvmridnundtxdm3ayqktmrt3lvdyys2e6g5g6\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7dzkqyzdeyyk7swhfnbrrhs4ocbhzfmxvlnomwklyczuf43o6ss6oe3fvmzncr4k5oj2yqbz3rzweejd6wzyt2pkf2q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-90acecb855b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlinkloadmore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdatakey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mgetloadmore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkloadmore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUAloadmore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mresploadmore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetloadmore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresploadmore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'load-more-data'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "outputId": "9b2c9764-0f69-4b6d-d0b7-ccd92d2cfcc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "source": [
        "# Write your code here\n",
        "#clean the text data\n",
        "from __future__ import division\n",
        "import nltk # re, pprint\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string \n",
        "\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"r\", encoding='utf-8') as file:\n",
        "  data = pd.read_csv(file)\n",
        "\n",
        "raw_title = [str(line) for line in data.review_title]\n",
        "raw_content = [str(line) for line in data.review_content]\n",
        "# Tokenize raw text in raw_title and raw_content\n",
        "tokens_title= [nltk.word_tokenize(title) for title in raw_title]\n",
        "tokens_content = [nltk.word_tokenize(item) for item in raw_content]\n",
        "\n",
        "# PROCCESSES OF CLEANING TEXT  \n",
        "def cleaning_text (text):\n",
        "    step1 = remove_noise(text)\n",
        "    step2 = remove_num (step1)\n",
        "    step3 = remove_stop (step2)\n",
        "    step4 = lowercase (step3)\n",
        "    step5 = stemming (step4)\n",
        "    step6 = lemma (step5)\n",
        "    return step6\n",
        "\n",
        "# (1) Remove noise, such as special characters and punctuations.      \n",
        "def remove_noise (list):\n",
        "    punct=[char for char in string.punctuation]\n",
        "    noise_removed = [word for word in list if word not in punct]\n",
        "    return noise_removed\n",
        "\n",
        "#(2) Remove numbers.\n",
        "def remove_num (list):\n",
        "    num_removed  = [word for word in list if not word.isnumeric()]\n",
        "    return num_removed\n",
        "\n",
        " # (3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "def remove_stop (list):\n",
        "    stopword_removed = [word for word in list if word not in stopwords.words('english')]\n",
        "    return stopword_removed\n",
        "\n",
        "#(4) Lowercase all texts\n",
        "def lowercase (list):\n",
        "    lowercase = [word.lower() for word in list]\n",
        "    return lowercase\n",
        "# (5) Stemming\n",
        "def stemming (list):\n",
        "    ps = PorterStemmer()\n",
        "    word_stemmed = [ps.stem(word) for word in list]\n",
        "    return word_stemmed\n",
        "# (6) Lemmatization.\n",
        "def lemma (list):\n",
        "    wnl =  WordNetLemmatizer ()\n",
        "    word_lemmatized = [wnl.lemmatize(word) for word in list]\n",
        "    return word_lemmatized\n",
        "\n",
        "# Cleaning title reviews and content review\n",
        "cleaned_title = [cleaning_text (sublist) for sublist in tokens_title]\n",
        "cleaned_content = [cleaning_text (sublist) for sublist in tokens_content]\n",
        "# Save cleaned text into new columns in dataframe\n",
        "df[\"cleaned_title\"] = cleaned_title\n",
        "df[\"cleaned_content\"] = cleaned_content\n",
        "df\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_title</th>\n",
              "      <th>review_id</th>\n",
              "      <th>review_date</th>\n",
              "      <th>review_nickname</th>\n",
              "      <th>review_content</th>\n",
              "      <th>review_rating</th>\n",
              "      <th>review_helpfulness</th>\n",
              "      <th>cleaned_title</th>\n",
              "      <th>cleaned_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a viewer that actually went to TIFF and wit...</td>\n",
              "      <td>rw5112402</td>\n",
              "      <td>10 September 2019</td>\n",
              "      <td>JF500</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>6,840 out of 7,862 found t...</td>\n",
              "      <td>[a, viewer, actual, went, tiff, wit, film, n't...</td>\n",
              "      <td>[i, person, saw, hype, claim, masterpiec, over...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outstanding movie with a haunting performance ...</td>\n",
              "      <td>rw5159304</td>\n",
              "      <td>3 October 2019</td>\n",
              "      <td>MihaVrhunc</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>3,375 out of 4,039 found t...</td>\n",
              "      <td>[outstand, movi, haunt, perform, best, charact...</td>\n",
              "      <td>[everi, movi, come, truli, make, impact, joaqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Only certain people can relate</td>\n",
              "      <td>rw5168360</td>\n",
              "      <td>7 October 2019</td>\n",
              "      <td>lesterarnoldpinto</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>2,676 out of 3,304 found t...</td>\n",
              "      <td>[onli, certain, peopl, relat]</td>\n",
              "      <td>[thi, movi, felt, alon, isol, truli, relat, yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Must have put a SMILE of Satisfaction on Heath...</td>\n",
              "      <td>rw5092831</td>\n",
              "      <td>1 September 2019</td>\n",
              "      <td>Chandler_Bing_</td>\n",
              "      <td>Truly a masterpiece, The Best film of 2019, on...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>3,166 out of 3,935 found t...</td>\n",
              "      <td>[must, put, smile, satisfact, heath, ledger, '...</td>\n",
              "      <td>[truli, masterpiec, the, best, film, one, best...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Hype is real</td>\n",
              "      <td>rw5160204</td>\n",
              "      <td>4 October 2019</td>\n",
              "      <td>kdagoulis26</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>2,352 out of 2,918 found t...</td>\n",
              "      <td>[the, hype, real]</td>\n",
              "      <td>[most, time, movi, anticip, like, end, fall, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>Spare me the \"best comic book movie ever\" nons...</td>\n",
              "      <td>rw5144576</td>\n",
              "      <td>26 September 2019</td>\n",
              "      <td>gatozangado</td>\n",
              "      <td>It premiered last night in Sydney and I was lu...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>102 out of 206 found this ...</td>\n",
              "      <td>[spare, ``, best, comic, book, movi, ever, '',...</td>\n",
              "      <td>[it, premier, last, night, sydney, i, lucki, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>Don't fall for the hype</td>\n",
              "      <td>rw5206952</td>\n",
              "      <td>23 October 2019</td>\n",
              "      <td>ninjashidan</td>\n",
              "      <td>I saw the joker last night and was highly disa...</td>\n",
              "      <td>1/10</td>\n",
              "      <td>88 out of 176 found this h...</td>\n",
              "      <td>[do, n't, fall, hype]</td>\n",
              "      <td>[i, saw, joker, last, night, highli, disappoin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>As movie well overrated - acting excellent</td>\n",
              "      <td>rw5175282</td>\n",
              "      <td>9 October 2019</td>\n",
              "      <td>dhllon</td>\n",
              "      <td>The depth of the story was lacking to much foc...</td>\n",
              "      <td>2/10</td>\n",
              "      <td>59 out of 114 found this h...</td>\n",
              "      <td>[a, movi, well, overr, act, excel]</td>\n",
              "      <td>[the, depth, stori, lack, much, focu, short, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Lives up to the hype</td>\n",
              "      <td>rw5158293</td>\n",
              "      <td>3 October 2019</td>\n",
              "      <td>i_like_jellycups</td>\n",
              "      <td>A breath of fresh airThink heath ledgers joker...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>99 out of 200 found this h...</td>\n",
              "      <td>[live, hype]</td>\n",
              "      <td>[a, breath, fresh, airthink, heath, ledger, jo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Arthur Fleck: a case study of a young person's...</td>\n",
              "      <td>rw5330561</td>\n",
              "      <td>18 December 2019</td>\n",
              "      <td>Fella_shibby</td>\n",
              "      <td>This film surpassed all other films in portray...</td>\n",
              "      <td>9/10</td>\n",
              "      <td>36 out of 66 found this he...</td>\n",
              "      <td>[arthur, fleck, case, studi, young, person, 's...</td>\n",
              "      <td>[thi, film, surpass, film, portray, slow, desc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>114 rows  9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          review_title  ...                                    cleaned_content\n",
              "0    As a viewer that actually went to TIFF and wit...  ...  [i, person, saw, hype, claim, masterpiec, over...\n",
              "1    Outstanding movie with a haunting performance ...  ...  [everi, movi, come, truli, make, impact, joaqu...\n",
              "2                       Only certain people can relate  ...  [thi, movi, felt, alon, isol, truli, relat, yo...\n",
              "3    Must have put a SMILE of Satisfaction on Heath...  ...  [truli, masterpiec, the, best, film, one, best...\n",
              "4                                     The Hype is real  ...  [most, time, movi, anticip, like, end, fall, s...\n",
              "..                                                 ...  ...                                                ...\n",
              "109  Spare me the \"best comic book movie ever\" nons...  ...  [it, premier, last, night, sydney, i, lucki, e...\n",
              "110                            Don't fall for the hype  ...  [i, saw, joker, last, night, highli, disappoin...\n",
              "111         As movie well overrated - acting excellent  ...  [the, depth, stori, lack, much, focu, short, a...\n",
              "112                               Lives up to the hype  ...  [a, breath, fresh, airthink, heath, ledger, jo...\n",
              "113  Arthur Fleck: a case study of a young person's...  ...  [thi, film, surpass, film, portray, slow, desc...\n",
              "\n",
              "[114 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab_type": "code",
        "outputId": "fb28254b-3ed3-45bf-ff33-1c628501c50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "# Write your code here\n",
        "#(1) Tag parts of Speech (POS) and  calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb)...\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from collections import Counter\n",
        "count_POStag = []\n",
        "for tokens in cleaned_title:\n",
        "    title_tagged = nltk.pos_tag(tokens)\n",
        "    #fd_title = nltk.FreqDist(tag for (token, tag) in title_tagged)\n",
        "    #print(fd_title)\n",
        "    count_POStag.append(Counter (tag for tokens, tag in title_tagged))\n",
        "for tokens in cleaned_content:\n",
        "    content_tagged = nltk.pos_tag(tokens)\n",
        "    #fd_title = nltk.FreqDist(tag for (token, tag) in title_tagged)\n",
        "    #print(fd_title)\n",
        "    count_POStag. append (Counter (tag for tokens, tag in content_tagged))\n",
        "    \n",
        "print(len(count_POStag))\n",
        "print(count_POStag)\n",
        "count_VB=0\n",
        "count_NN=0\n",
        "count_ADJ=0\n",
        "count_ADV=0\n",
        "for pack in count_POStag:\n",
        "  for item in pack:\n",
        "    if item ==\"VB\" or item==\"VBG\" or item==\"VBD\" or item==\"VBP\" or item==\"VBZ\":\n",
        "      count_VB+=1\n",
        "    elif item ==\"NN\" or item==\"NNS\" or item ==\"NNP\" or item==\"NNPS\":\n",
        "      count_NN+=1\n",
        "    elif item ==\"JJ\" or item ==\"JJR\" or item==\"JJS\":\n",
        "      count_ADJ+=1\n",
        "    elif item ==\"RB\" or item==\"RBR\" or item==\"RBS\":\n",
        "      count_ADV+=1\n",
        "    else:\n",
        "      pass\n",
        "print(\"The total verbs:\", count_VB)\n",
        "print(\"The total nouns:\", count_NN)\n",
        "print(\"The total adjectives:\", count_ADJ)\n",
        "print(\"The total adverbs:\", count_ADV)\n",
        "#for n in count_POStag:\n",
        " # for items in count_POStag\n",
        " # hist[key]=hist.get(valu=)+1\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "228\n",
            "[Counter({'NN': 9, 'JJ': 3, 'DT': 1, 'VBD': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 5, 'JJS': 1, 'VB': 1, 'RB': 1, 'VBN': 1}), Counter({'NN': 3, 'JJ': 1}), Counter({'NN': 4, 'RB': 2, 'MD': 1, 'VB': 1, 'JJ': 1, 'POS': 1}), Counter({'DT': 1, 'NN': 1, 'JJ': 1}), Counter({'NN': 2}), Counter({'RB': 1, 'VB': 1, 'WRB': 1, 'JJ': 1, 'VBP': 1}), Counter({'NN': 2, 'VBD': 1, 'JJ': 1}), Counter({'NN': 3, 'DT': 1, 'RB': 1, 'VBD': 1}), Counter({'NN': 6, 'POS': 1, 'JJS': 1, 'VBZ': 1, ':': 1, 'VBD': 1}), Counter({'JJ': 1, 'NN': 1}), Counter({'JJ': 2, 'NN': 1}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 4, 'RB': 2, 'VB': 1}), Counter({'NN': 2}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 2}), Counter({'NN': 2}), Counter({'VB': 2, 'VBP': 1, 'RB': 1}), Counter({'NN': 2, 'MD': 1, 'VB': 1}), Counter({'NN': 3, 'CD': 1, 'JJS': 1, 'VBP': 1, 'RB': 1, 'VBN': 1}), Counter({'IN': 1}), Counter({'NN': 2}), Counter({'IN': 1, 'DT': 1, 'NN': 1}), Counter({'NN': 1}), Counter({'NN': 1}), Counter({'NN': 2, 'DT': 1}), Counter({'JJ': 2, 'VBZ': 1, 'RB': 1, 'VB': 1}), Counter({'``': 1, 'DT': 1, 'VBZ': 1, 'NN': 1, \"''\": 1}), Counter({'NN': 2}), Counter({'NN': 2, 'JJ': 1}), Counter({'NN': 3, 'CC': 1, 'JJS': 1, 'VBP': 1, 'RB': 1, 'VBN': 1}), Counter({'NN': 2}), Counter({'RB': 2, 'NN': 2, 'JJ': 1, 'POS': 1, '``': 1, 'JJS': 1, \"''\": 1}), Counter({'JJ': 3, 'VBP': 2, 'NN': 2, 'IN': 1}), Counter({'NN': 3, 'JJ': 1, 'IN': 1}), Counter({'NN': 5, 'JJ': 2, 'VB': 1, 'CD': 1, 'JJS': 1, 'NNS': 1, 'VBP': 1}), Counter({'NN': 3, 'VBP': 2, 'VBD': 1, 'RB': 1, 'JJ': 1}), Counter({'JJ': 4, 'NN': 4, 'VBP': 3, 'VBN': 2, 'RB': 2, 'NNS': 1, 'IN': 1, 'PRP': 1}), Counter({'NN': 3, 'JJ': 1, '``': 1, \"''\": 1}), Counter({'NN': 3, 'JJ': 1, 'DT': 1}), Counter({'NN': 1}), Counter({'NN': 5, 'VBP': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 3, 'JJ': 1, 'VBP': 1, 'IN': 1}), Counter({'NNS': 1, 'NN': 1}), Counter({'JJ': 3, 'NN': 2}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 5, 'VBD': 1}), Counter({'NN': 2, 'VBZ': 1, 'WRB': 1, 'DT': 1, 'JJ': 1, 'MD': 1, 'VB': 1}), Counter({'NN': 4, 'VBD': 2, 'JJS': 1, 'DT': 1}), Counter({'NN': 2, 'VB': 1}), Counter({'WP': 1, 'VBZ': 1, 'JJ': 1, 'NN': 1}), Counter({'NN': 2}), Counter({'NN': 4, 'RB': 2, 'JJ': 2, 'NNS': 1, 'VBP': 1}), Counter({'NN': 1, 'POS': 1, 'PRP': 1}), Counter({'NN': 2}), Counter({'NN': 2, 'VB': 2, 'RB': 1, 'JJ': 1, 'VBP': 1}), Counter({'RB': 2, ':': 1}), Counter({'NN': 2, 'JJS': 1, 'IN': 1, 'DT': 1}), Counter({'RB': 1, 'JJ': 1}), Counter({'VBP': 1, 'RB': 1, 'VB': 1, 'NN': 1}), Counter({'NN': 4, 'JJ': 2}), Counter({'NN': 2, 'RB': 1}), Counter({'NN': 4, 'VBD': 1, 'IN': 1}), Counter({'NN': 1}), Counter({'NN': 1}), Counter({'NN': 3, 'JJ': 1}), Counter({'NN': 4, 'DT': 1, 'JJS': 1, 'VBN': 1, 'VBD': 1}), Counter({'NN': 1, 'VBP': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 2}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 1}), Counter({'RB': 1, 'NN': 1}), Counter({'NN': 2, 'JJ': 1}), Counter({'NN': 2, 'JJ': 1}), Counter({'NN': 2, 'JJ': 1}), Counter({'NN': 2}), Counter({'NN': 2}), Counter({'NN': 6, 'IN': 1, 'JJS': 1}), Counter({'VBP': 1, 'JJ': 1, 'CD': 1, 'RB': 1, 'IN': 1, 'VB': 1}), Counter({'NNS': 1, 'NN': 1}), Counter({'NN': 2}), Counter({'NN': 2, 'DT': 1}), Counter({'NN': 1}), Counter({'NN': 3}), Counter({'NN': 2}), Counter({'JJ': 2, 'NN': 2, 'DT': 1}), Counter({'NN': 4, 'JJ': 1, ':': 1}), Counter({'NN': 2}), Counter({'NN': 5, 'IN': 1, 'RB': 1, 'JJS': 1, 'JJ': 1}), Counter({'NN': 4, 'RB': 1, 'VBD': 1, 'CD': 1}), Counter({'NNS': 1, 'VBP': 1}), Counter({'CD': 1, 'DT': 1, 'JJS': 1, 'NN': 1, 'RB': 1}), Counter({'NN': 4, 'JJ': 2}), Counter({'IN': 2, 'NN': 2}), Counter({'NN': 1}), Counter({'NN': 2, ':': 1}), Counter({'NN': 1}), Counter({'NN': 1, 'VBD': 1}), Counter({'NN': 2}), Counter({'NN': 9, 'VBP': 1, 'JJ': 1}), Counter({'NN': 3, 'VBP': 1, 'RB': 1}), Counter({'NN': 4, 'POS': 1}), Counter({'NN': 1}), Counter({'NN': 2, 'DT': 1}), Counter({'NN': 3, 'VBD': 1, 'JJ': 1}), Counter({'RB': 1, 'JJ': 1, 'NN': 1}), Counter({'NN': 2, 'RB': 1}), Counter({'JJ': 2, 'NN': 2, '``': 1, 'JJS': 1, 'RB': 1, \"''\": 1, 'NNS': 1}), Counter({'VBP': 1, 'RB': 1, 'VB': 1, 'NN': 1}), Counter({'NN': 3, 'DT': 1, 'RB': 1, 'IN': 1}), Counter({'JJ': 1, 'NN': 1}), Counter({'NN': 8, 'JJ': 2, 'RB': 1, 'POS': 1}), Counter({'NN': 57, 'JJ': 19, 'VBP': 7, 'RB': 6, 'VB': 3, 'NNS': 3, 'VBN': 3, 'POS': 3, 'VBD': 2, 'DT': 2, 'IN': 2, 'JJS': 2, 'MD': 2, 'VBZ': 1, 'WRB': 1}), Counter({'NN': 26, 'JJ': 10, 'RB': 4, 'VBP': 3, 'POS': 3, 'NNS': 1, ':': 1, 'MD': 1, 'VB': 1, 'VBD': 1, 'JJS': 1, 'VBN': 1}), Counter({'NN': 18, 'JJ': 9, 'VBP': 5, 'NNS': 2, 'VBD': 1, 'PRP': 1, 'DT': 1, 'CC': 1, 'CD': 1, 'RBR': 1, 'IN': 1, 'RB': 1, 'VB': 1}), Counter({'NN': 20, 'JJ': 8, 'JJS': 3, 'VB': 2, 'VBP': 2, 'VBZ': 1, 'DT': 1, 'CD': 1, ':': 1, 'CC': 1, 'VBG': 1, 'RB': 1}), Counter({'NN': 26, 'JJ': 10, 'VBP': 4, 'JJS': 2, 'NNS': 2, 'IN': 2, '``': 2, \"''\": 2, 'RB': 2, 'PRP': 1, 'VBN': 1, 'JJR': 1, 'VBZ': 1, 'DT': 1}), Counter({'NN': 27, 'JJ': 9, 'IN': 4, 'POS': 4, 'DT': 3, 'VBD': 3, 'VB': 2, 'VBP': 2, 'FW': 1, 'PRP': 1, 'VBZ': 1}), Counter({'NN': 27, 'JJ': 19, 'VBP': 8, 'RB': 8, 'VB': 7, 'CC': 4, 'DT': 3, 'VBD': 3, 'RBS': 2, 'NNS': 1, 'MD': 1, 'VBN': 1, 'VBZ': 1, 'IN': 1, 'PRP': 1, 'VBG': 1, 'JJS': 1}), Counter({'NN': 19, 'VB': 4, 'VBP': 3, 'JJ': 3, 'PRP': 1, 'VBZ': 1, 'FW': 1, 'MD': 1, 'POS': 1, 'CC': 1, 'RB': 1, 'DT': 1, 'IN': 1}), Counter({'NN': 77, 'JJ': 37, 'RB': 7, 'POS': 6, 'VBP': 5, 'VB': 4, 'IN': 3, 'PRP': 3, 'MD': 3, 'JJS': 3, 'VBN': 2, 'VBZ': 2, 'CC': 1, 'VBD': 1, 'DT': 1, 'NNS': 1, 'CD': 1, 'WP': 1}), Counter({'NN': 29, 'JJ': 14, 'VB': 6, 'VBZ': 3, 'VBP': 2, 'VBD': 2, 'RB': 2, 'PRP': 1, 'POS': 1, 'CC': 1, 'NNP': 1, 'TO': 1, 'RP': 1, 'JJS': 1}), Counter({'NN': 20, 'JJ': 13, 'RB': 9, 'VB': 7, 'VBP': 5, 'MD': 2, 'NNS': 1, 'VBD': 1, 'CD': 1, '``': 1, \"''\": 1}), Counter({'NN': 31, 'JJ': 14, 'VBP': 6, 'VBD': 5, 'RB': 5, 'NNS': 3, 'VB': 3, 'PRP': 2, 'IN': 2, 'DT': 1, '``': 1, \"''\": 1, 'VBZ': 1, 'MD': 1, 'JJS': 1}), Counter({'NN': 10, 'JJ': 2, 'VBP': 2, 'DT': 1, 'NNS': 1}), Counter({'NN': 4, 'JJ': 3, 'DT': 1, 'VBP': 1, 'RB': 1}), Counter({'NN': 7, 'JJ': 2, 'VBP': 1}), Counter({'NN': 65, 'JJ': 32, 'VBD': 9, 'VB': 6, 'VBP': 5, 'IN': 3, 'RB': 3, 'MD': 3, 'DT': 2, ':': 1, 'WDT': 1, 'WP': 1, 'TO': 1, 'NNS': 1, 'CC': 1}), Counter({'NN': 34, 'JJ': 10, 'VBP': 4, 'NNS': 2, 'RB': 2, 'POS': 2, 'VB': 1, 'PRP': 1, 'IN': 1}), Counter({'NN': 38, 'JJ': 18, 'VB': 4, 'RB': 4, 'VBP': 4, 'VBD': 3, 'CC': 2, 'IN': 2, 'NNS': 1, 'MD': 1, 'DT': 1, 'RBR': 1}), Counter({'NN': 89, 'JJ': 39, 'VBP': 12, 'VB': 10, 'RB': 8, 'POS': 7, 'NNS': 4, 'VBZ': 3, 'IN': 3, 'CD': 3, 'MD': 2, 'JJS': 2, 'VBD': 2, 'PRP': 1, 'VBN': 1, 'VBG': 1}), Counter({'NN': 81, 'JJ': 36, 'RB': 16, 'VBD': 14, 'VB': 12, 'VBP': 9, 'VBN': 4, 'MD': 4, 'POS': 4, 'PRP': 4, 'VBZ': 4, 'DT': 3, 'IN': 3, 'CD': 2, ':': 1, 'EX': 1, 'WP': 1, 'NNS': 1, 'CC': 1}), Counter({'NN': 40, 'JJ': 15, 'VBP': 7, 'VBN': 4, 'VBD': 3, 'IN': 3, 'DT': 3, 'VBZ': 2, 'NNS': 2, 'POS': 2, 'RB': 2, 'FW': 1, 'JJS': 1, 'VB': 1, 'CD': 1, 'RBR': 1, 'PRP': 1, 'MD': 1}), Counter({'NN': 32, 'JJ': 8, 'VBP': 6, 'IN': 5, 'VB': 4, 'RB': 3, 'CD': 3, 'VBD': 2, 'DT': 1, 'MD': 1, 'POS': 1}), Counter({'NN': 27, 'JJ': 11, 'VBP': 8, ':': 3, 'IN': 3, 'VBD': 2, 'VBN': 2, 'CC': 1, 'JJS': 1, 'POS': 1, 'RBR': 1, 'RB': 1, 'MD': 1, 'VB': 1}), Counter({'NN': 33, 'VBP': 5, 'JJ': 5, 'VB': 4, 'RB': 4, 'NNS': 4, 'VBD': 4, 'CD': 3, 'PRP$': 1, 'TO': 1, 'JJS': 1, 'DT': 1, 'PRP': 1}), Counter({'NN': 32, 'JJ': 13, 'DT': 5, 'VB': 3, 'VBP': 3, 'NNS': 2, 'RB': 2, 'VBD': 2, 'IN': 2, 'MD': 2, 'POS': 1}), Counter({'NN': 32, 'JJ': 13, 'DT': 5, 'VB': 3, 'VBP': 3, 'NNS': 2, 'RB': 2, 'VBD': 2, 'IN': 2, 'MD': 2, 'POS': 1}), Counter({'NN': 52, 'JJ': 21, 'VBP': 15, 'VB': 8, 'IN': 5, 'PRP': 4, 'RB': 4, 'VBZ': 3, 'POS': 2, 'DT': 2, 'NNS': 2, 'VBN': 2, 'JJR': 2, 'MD': 2, 'VBD': 1, 'JJS': 1, 'EX': 1, 'WDT': 1, '``': 1, \"''\": 1, 'CD': 1}), Counter({'NN': 30, 'JJ': 17, 'VBD': 5, 'RB': 5, 'VBP': 4, 'DT': 4, 'NNS': 3, 'POS': 3, 'IN': 3, 'WRB': 2, 'VB': 1, 'VBZ': 1, 'PRP': 1}), Counter({'NN': 135, 'JJ': 47, 'IN': 11, 'RB': 10, 'VBP': 10, 'DT': 9, 'VB': 8, 'VBD': 8, 'MD': 5, 'VBZ': 3, 'JJR': 2, 'PRP': 2, 'VBG': 2, 'POS': 2, 'RBR': 2, 'NNS': 1, 'FW': 1, ':': 1, 'CD': 1, 'WP$': 1}), Counter({'NN': 27, 'JJ': 8, 'RB': 8, 'IN': 5, 'VB': 5, 'VBP': 4, 'NNS': 2, 'DT': 2, 'MD': 2, '``': 1, \"''\": 1, 'VBN': 1, 'FW': 1, 'POS': 1, 'VBD': 1, 'CC': 1, 'RBR': 1, 'VBZ': 1}), Counter({'NN': 30, 'DT': 5, 'JJ': 4, \"''\": 1, 'CD': 1, 'VBG': 1, 'PRP': 1, 'VBP': 1}), Counter({'NN': 27, 'JJ': 10, 'VBD': 6, 'DT': 2, 'VBP': 2, 'RB': 2, 'IN': 1, 'FW': 1, 'CC': 1, 'CD': 1, 'PRP$': 1, 'JJS': 1}), Counter({'NN': 15, 'VBP': 4, 'JJ': 4, 'IN': 3, 'VB': 2, 'DT': 1, 'PRP': 1, 'NNS': 1, 'RB': 1, '``': 1, \"''\": 1, 'VBZ': 1, 'JJR': 1, 'POS': 1}), Counter({'NN': 116, 'JJ': 58, 'VBP': 27, 'RB': 19, 'IN': 16, 'VB': 14, '``': 7, 'VBD': 6, 'NNS': 6, 'MD': 4, 'VBN': 4, \"''\": 3, 'POS': 3, 'VBZ': 3, 'RP': 2, 'DT': 1, 'CD': 1, 'JJR': 1, 'PRP$': 1, 'RBR': 1, 'JJS': 1, 'PRP': 1}), Counter({'NN': 57, 'JJ': 29, 'VBP': 13, 'RB': 6, 'VBN': 5, 'VB': 3, 'VBD': 3, 'IN': 3, 'MD': 2, 'JJR': 2, 'CC': 1, 'DT': 1, 'JJS': 1, ':': 1, 'PRP': 1, 'NNS': 1}), Counter({'NN': 60, 'JJ': 17, 'DT': 7, 'VB': 7, 'CD': 6, 'RB': 4, 'VBP': 4, 'POS': 3, 'JJS': 3, 'IN': 3, 'MD': 3, 'JJR': 3, 'VBD': 2, 'VBN': 1, 'VBZ': 1, 'NNP': 1, 'NNS': 1, 'RP': 1}), Counter({'NN': 198, 'JJ': 74, 'VBP': 24, 'RB': 22, 'VB': 19, 'IN': 12, 'POS': 10, 'VBD': 7, '``': 6, \"''\": 6, 'CD': 5, 'DT': 5, 'CC': 5, 'VBN': 4, 'MD': 4, 'JJS': 2, 'VBG': 2, 'NNS': 2, 'FW': 2, 'TO': 2, 'VBZ': 1, 'NNP': 1, 'PRP': 1, ':': 1}), Counter({'NN': 10, 'JJS': 4, 'JJ': 3, 'VBP': 3, 'RB': 2, 'VB': 2, 'NNS': 1, 'JJR': 1, 'IN': 1, 'CD': 1}), Counter({'NN': 68, 'JJ': 24, 'VBP': 11, 'RB': 8, 'IN': 5, 'POS': 5, 'VB': 5, 'MD': 3, 'VBN': 2, 'NNS': 2, 'WP$': 1, 'RBR': 1, 'VBZ': 1}), Counter({'NN': 44, 'JJ': 20, 'VB': 10, 'VBP': 6, 'MD': 5, 'NNS': 3, 'RB': 3, 'IN': 3, 'CD': 3, 'PRP': 2, 'VBD': 1, 'CC': 1, 'DT': 1, '``': 1, \"''\": 1, 'POS': 1, 'FW': 1, 'EX': 1, 'VBG': 1}), Counter({'NN': 80, 'JJ': 28, 'VBP': 10, 'VB': 7, 'RB': 6, 'NNS': 4, 'IN': 4, '``': 3, 'CD': 3, 'POS': 3, 'MD': 3, 'CC': 2, 'VBZ': 2, 'VBD': 2, \"''\": 1, 'JJR': 1, 'DT': 1, 'VBN': 1, 'PRP': 1, 'JJS': 1, ':': 1, 'WP': 1, 'EX': 1}), Counter({'NN': 9, 'CD': 2, 'JJS': 2, 'VBZ': 1, 'VBP': 1, 'RB': 1}), Counter({'NN': 97, 'JJ': 36, 'RB': 16, 'VBP': 14, 'IN': 11, 'VB': 10, 'VBD': 7, 'PRP': 6, 'DT': 5, 'MD': 4, 'VBN': 4, 'CC': 3, 'CD': 3, 'RP': 3, 'VBZ': 2, 'JJS': 2, 'POS': 2, 'JJR': 1, 'RBR': 1, 'NNS': 1}), Counter({'NN': 21, 'JJ': 6, 'VB': 3, 'POS': 2, 'IN': 2, 'VBP': 2, 'VBD': 1, 'MD': 1, 'NNS': 1}), Counter({'NN': 12, 'JJ': 5, 'RB': 5, 'VBP': 4, 'VBD': 2, 'VB': 2, 'IN': 2, 'NNS': 1, 'DT': 1, 'VBN': 1, ':': 1}), Counter({'NN': 78, 'JJ': 25, 'RB': 7, 'VB': 7, 'VBP': 7, 'VBD': 5, 'IN': 3, 'PRP': 2, 'VBZ': 2, 'MD': 2, 'DT': 2, 'VBN': 2, 'FW': 1, 'JJR': 1}), Counter({'NN': 17, 'JJ': 10, 'VBP': 2, 'VBD': 2, 'RB': 2, 'VBZ': 2, 'POS': 1, 'IN': 1, 'NNS': 1, 'DT': 1, 'NNP': 1, 'RBR': 1, ':': 1, 'PRP': 1, 'VB': 1}), Counter({'NN': 23, 'JJ': 6, 'VBD': 4, 'RB': 3, 'IN': 3, 'WP': 1, 'VB': 1, 'FW': 1, 'CC': 1}), Counter({'NN': 16, 'JJ': 9, 'VB': 5, 'VBP': 3, 'RB': 3, 'VBD': 2, 'IN': 2, 'NNS': 1, 'MD': 1, 'CD': 1}), Counter({'NN': 17, 'JJ': 6, 'VBP': 4, 'VBD': 3, 'CD': 3, 'VB': 3, 'PRP': 2, 'RP': 1, 'JJS': 1, 'VBN': 1, 'RB': 1, 'CC': 1}), Counter({'NN': 177, 'JJ': 78, 'VB': 31, 'VBP': 26, 'RB': 22, 'MD': 16, 'IN': 15, 'PRP': 13, 'VBD': 11, 'CC': 9, 'POS': 9, 'CD': 6, 'VBZ': 5, 'DT': 5, 'NNS': 4, 'JJR': 2, 'RBR': 1, 'EX': 1, 'WDT': 1, 'VBN': 1, 'FW': 1}), Counter({'NN': 19, 'JJ': 7, ':': 3, 'DT': 2, 'VBP': 2, 'POS': 1, 'CC': 1, 'IN': 1, 'VB': 1, 'JJS': 1, 'WP': 1}), Counter({'NN': 19, 'JJ': 13, 'RB': 4, 'VBD': 3, 'VBP': 2, 'VB': 2, 'NNS': 1, 'PRP': 1, 'CC': 1, 'VBN': 1}), Counter({'NN': 10, 'JJ': 5, 'RB': 2, 'VBP': 2, 'VBD': 1}), Counter({'NN': 33, 'JJ': 16, 'RB': 6, 'VB': 6, 'VBP': 4, 'IN': 4, 'NNS': 3, 'VBD': 3, 'POS': 2, '``': 2, \"''\": 2, 'CD': 2, 'DT': 2, 'MD': 2, 'FW': 1, 'WP$': 1, 'VBZ': 1}), Counter({'NN': 139, 'JJ': 42, 'VBP': 19, 'RB': 17, 'VB': 12, 'VBD': 10, 'IN': 9, 'CD': 8, 'POS': 5, 'VBZ': 5, 'MD': 4, 'PRP': 3, 'VBN': 3, 'RBR': 2, 'NNS': 2, 'DT': 2, 'FW': 1, 'JJR': 1, 'EX': 1, 'RBS': 1, 'JJS': 1, 'PDT': 1, 'NNP': 1}), Counter({'NN': 26, 'VBP': 6, 'JJ': 5, 'VB': 2, 'POS': 2, 'VBZ': 1, 'RB': 1}), Counter({'NN': 25, 'JJ': 11, 'VBP': 5, 'RB': 4, 'IN': 3, 'VBD': 3, 'CD': 2, 'VB': 2, 'POS': 1, 'VBN': 1, 'NNS': 1, 'MD': 1}), Counter({'NN': 21, 'JJ': 10, 'VB': 5, 'POS': 5, 'VBP': 3, 'PRP': 2, 'VBZ': 2, 'RB': 2, 'IN': 2, 'CD': 1, 'VBD': 1, 'JJR': 1, 'WRB': 1, 'CC': 1}), Counter({'NN': 11, 'VBP': 2, 'JJ': 2, 'DT': 1, 'POS': 1, 'VBN': 1}), Counter({'NN': 14, 'RB': 4, 'VB': 4, 'VBP': 3, 'JJ': 3, ':': 2, 'VBD': 2, 'JJS': 1, 'VBN': 1, 'MD': 1, 'IN': 1, 'CC': 1, 'CD': 1}), Counter({'NN': 16, 'JJ': 5, 'IN': 3, 'VBP': 2, 'VB': 2, 'PRP': 1, 'VBZ': 1, 'CC': 1, 'RB': 1, 'POS': 1, 'VBN': 1, 'TO': 1}), Counter({'NN': 126, 'JJ': 59, 'RB': 30, 'VBP': 20, 'VB': 16, 'IN': 15, 'POS': 11, 'VBD': 10, 'VBZ': 7, 'PRP': 6, 'CD': 6, 'DT': 4, 'NNS': 4, 'MD': 3, 'JJR': 2, 'JJS': 2, 'FW': 1, 'CC': 1, 'VBN': 1, 'TO': 1, 'WP': 1, '``': 1, \"''\": 1}), Counter({'NN': 1}), Counter({'NN': 44, 'JJ': 16, 'VBP': 13, 'RB': 6, 'VB': 3, 'DT': 3, 'IN': 3, 'VBN': 2, 'VBD': 2, 'VBZ': 2, 'PRP': 1, 'NNS': 1, 'POS': 1, 'EX': 1, 'RBR': 1}), Counter({'NN': 9, 'JJ': 5, 'VBD': 3, 'MD': 2, 'VB': 2, ':': 2, 'VBN': 2, 'DT': 1, 'PRP': 1, 'RB': 1, 'VBP': 1}), Counter({'NN': 41, 'JJ': 7, 'IN': 5, 'VBP': 3, 'RB': 2, 'POS': 2, 'CC': 2, 'VBD': 2, 'VBN': 2, 'JJS': 1, 'PRP': 1, 'VBZ': 1, 'DT': 1, 'NNS': 1, 'TO': 1, 'VB': 1, 'WRB': 1, 'RBR': 1}), Counter({'NN': 13, 'JJ': 5, 'IN': 4, 'RB': 2, 'DT': 2, 'VBP': 1, 'VBN': 1, 'FW': 1, 'VB': 1, 'VBZ': 1}), Counter({'NN': 18, 'JJ': 5, 'VBP': 3, 'RB': 2, 'VB': 2, 'IN': 1, 'PRP': 1, 'VBZ': 1, 'NNS': 1, 'DT': 1, 'VBN': 1, 'CD': 1}), Counter({'NN': 94, 'JJ': 30, 'VBP': 15, 'RB': 14, 'IN': 9, 'DT': 7, 'VBD': 6, 'NNS': 6, 'VBZ': 5, 'POS': 4, 'VB': 4, 'FW': 3, 'PRP': 2, 'EX': 1, 'JJR': 1, 'CC': 1}), Counter({'NN': 18, 'JJ': 7, 'VB': 4, 'RB': 2, 'CC': 2, 'PRP': 2, 'POS': 2, 'VBD': 1, 'VBP': 1, 'DT': 1, 'VBZ': 1, 'IN': 1, 'WP': 1}), Counter({'NN': 37, 'JJ': 22, 'VBP': 6, 'IN': 5, 'VB': 4, 'DT': 3, 'CD': 3, 'NNS': 2, 'VBN': 2, 'POS': 2, 'CC': 2, 'RB': 2, 'MD': 1, 'JJS': 1}), Counter({'NN': 8, 'JJS': 2, 'CD': 1, 'VBN': 1, 'POS': 1, 'JJ': 1}), Counter({'NN': 96, 'JJ': 35, 'VBP': 10, 'VB': 8, 'RB': 7, 'IN': 6, 'VBZ': 4, 'VBD': 4, 'PRP': 4, 'MD': 4, 'NNS': 4, ':': 2, 'POS': 2, 'JJS': 2, 'VBN': 2, 'WDT': 2, 'RBR': 1, 'JJR': 1, 'DT': 1, 'CC': 1, '``': 1, \"''\": 1, 'CD': 1}), Counter({'NN': 26, 'JJ': 5, 'VBP': 3, 'VBD': 3, 'IN': 3, 'VB': 2, 'DT': 2, 'CD': 1, 'NNS': 1, 'VBN': 1, 'PRP': 1, 'RB': 1, 'NNP': 1}), Counter({'NN': 29, 'JJ': 4, 'VBP': 4, 'VBD': 3, 'IN': 2, 'RB': 2, 'PRP': 1, 'VBZ': 1, 'VBN': 1, 'NNS': 1, 'MD': 1, 'VB': 1, 'DT': 1, 'POS': 1}), Counter({'NN': 13, 'DT': 2, 'VBZ': 2, 'JJ': 2, 'RB': 1, 'VBN': 1, 'VBD': 1, 'RBR': 1, 'IN': 1}), Counter({'NN': 4, 'JJS': 1, 'JJ': 1, 'VBP': 1}), Counter({'NN': 21, 'JJ': 4, 'RBR': 2, 'DT': 1, 'TO': 1, 'VB': 1, '``': 1, 'CD': 1, 'JJS': 1, \"''\": 1}), Counter({'NN': 82, 'JJ': 36, 'VBP': 11, 'RB': 8, 'VBD': 7, 'IN': 6, 'POS': 5, 'VBN': 3, 'DT': 3, 'VB': 3, 'CD': 2, 'JJS': 2, 'MD': 2, 'NNS': 2, 'PRP': 1, 'CC': 1}), Counter({'NN': 14, 'JJ': 3, 'POS': 1, ':': 1, 'VBD': 1, 'RB': 1, 'VBN': 1, 'VBZ': 1, 'DT': 1, 'VB': 1, 'IN': 1}), Counter({'NN': 27, 'JJ': 6, 'VBP': 3, 'NNS': 2, 'VBD': 2, 'VBN': 1, 'PRP': 1, 'RB': 1, 'VB': 1, 'IN': 1}), Counter({'NN': 28, 'JJ': 7, '``': 5, \"''\": 5, 'RB': 2, 'IN': 2, 'RBR': 2, 'VBP': 1, 'VB': 1, 'VBD': 1, 'VBN': 1}), Counter({'NN': 14, 'JJ': 3, 'WP': 1, 'VBZ': 1, 'VBP': 1, 'VB': 1, 'PRP': 1, 'VBD': 1, 'RB': 1, 'VBN': 1, 'IN': 1, 'DT': 1}), Counter({'NN': 7, 'VBP': 5, 'RB': 2, 'JJ': 2, 'NNS': 1, 'DT': 1}), Counter({'NN': 40, 'JJ': 29, 'VB': 7, 'MD': 6, 'RB': 6, 'VBP': 5, 'DT': 3, 'IN': 3, 'POS': 3, 'VBZ': 2, 'VBN': 2, 'WP': 1, 'WDT': 1, 'JJR': 1, 'CD': 1, 'NNS': 1}), Counter({'NN': 14, 'FW': 1, '$': 1, 'CD': 1, 'JJ': 1, 'VBZ': 1, 'JJR': 1}), Counter({'NN': 111, 'JJ': 31, 'RB': 19, 'VBP': 19, 'IN': 14, 'VB': 12, 'MD': 6, 'VBD': 5, 'CC': 5, 'VBN': 4, 'PRP': 4, 'DT': 3, 'POS': 2, '``': 2, \"''\": 2, 'CD': 2, 'WRB': 1, 'VBZ': 1, 'NNS': 1, 'RBR': 1}), Counter({'NN': 55, 'JJ': 17, 'VB': 8, 'IN': 7, 'RB': 6, 'VBP': 5, ':': 5, 'PRP': 4, 'VBD': 3, 'VBZ': 3, 'WP': 2, 'JJS': 2, 'CC': 2, 'FW': 1, 'VBN': 1, '``': 1, \"''\": 1, 'POS': 1, 'MD': 1, 'DT': 1}), Counter({'NN': 6, 'JJ': 2, 'DT': 1, 'VB': 1, 'WRB': 1}), Counter({'NN': 26, 'RB': 11, 'JJ': 11, 'VBP': 7, 'VB': 7, 'VBD': 2, 'MD': 2, 'WRB': 1, 'CD': 1, 'POS': 1, 'PRP': 1, 'VBN': 1, 'NNS': 1}), Counter({'NN': 103, 'JJ': 43, 'VBP': 20, 'VB': 12, 'IN': 9, 'VBD': 8, 'DT': 8, 'RB': 7, 'NNS': 6, 'PRP': 5, 'VBN': 4, 'VBZ': 4, 'CC': 4, 'CD': 3, 'MD': 2, 'JJS': 2, 'VBG': 1, '``': 1, \"''\": 1, 'WRB': 1, 'FW': 1}), Counter({'NN': 57, 'JJ': 22, 'RB': 9, 'VBZ': 6, '``': 4, \"''\": 4, 'VBN': 3, 'POS': 3, 'VBP': 3, 'VB': 3, 'VBD': 2, 'DT': 2, ':': 2, 'JJS': 1, 'IN': 1, 'NNS': 1, 'VBG': 1, 'MD': 1, 'PRP': 1, 'FW': 1, 'NNP': 1}), Counter({'NN': 47, 'VBP': 11, 'VB': 11, 'JJ': 11, 'RB': 9, 'MD': 6, 'PRP': 4, 'IN': 4, 'NNS': 3, 'VBD': 3, 'VBZ': 3, 'JJS': 3, 'CD': 1, 'POS': 1, 'CC': 1}), Counter({'NN': 10, 'JJ': 2, 'DT': 1, 'CD': 1, 'POS': 1}), Counter({'NN': 15, 'JJ': 9, 'IN': 3, 'RB': 2, 'DT': 2, 'PRP': 1, 'VBN': 1, 'VBD': 1, 'NNS': 1, 'VBP': 1}), Counter({'NN': 17, 'JJ': 4, 'VBD': 2, 'VBP': 2, 'RB': 2, 'VB': 1}), Counter({'NN': 3, 'VB': 2, 'VBP': 1, 'JJ': 1}), Counter({'NN': 5, 'JJ': 3, 'VBP': 2, 'RB': 1, 'VB': 1, 'POS': 1, '``': 1, \"''\": 1, 'VBD': 1}), Counter({'NN': 16, 'JJ': 8, 'JJS': 4, 'RBS': 3, 'MD': 2, 'VB': 2, 'RB': 1, 'VBD': 1, 'NNS': 1, 'VBP': 1, 'IN': 1}), Counter({'NN': 30, 'VB': 10, 'JJ': 9, 'RB': 7, 'MD': 5, 'VBP': 4, 'DT': 2, 'IN': 2, 'POS': 2, 'TO': 1, 'NNS': 1, 'FW': 1, ':': 1, 'WRB': 1}), Counter({'NN': 99, 'JJ': 28, 'VBP': 6, 'RB': 6, 'IN': 6, 'DT': 4, 'VB': 4, 'VBD': 4, 'PRP': 4, 'VBZ': 4, 'CD': 2, ':': 2, 'MD': 1, 'VBN': 1, 'WP': 1, 'CC': 1, 'NNS': 1}), Counter({'NN': 37, 'JJ': 12, 'VBP': 5, 'VB': 5, 'POS': 4, 'VBD': 3, '``': 3, 'DT': 3, 'IN': 2, 'RB': 2, 'CC': 2, 'NNS': 1, 'WRB': 1, \"''\": 1, 'MD': 1, 'VBZ': 1, 'VBN': 1, 'CD': 1}), Counter({'NN': 78, 'JJ': 34, 'VBP': 6, 'RB': 6, 'FW': 4, 'IN': 4, 'VBD': 4, 'VB': 4, 'NNS': 3, 'POS': 2, 'VBN': 2, 'WP$': 2, '``': 1, \"''\": 1, 'RBR': 1, 'MD': 1}), Counter({'NN': 113, 'JJ': 29, 'VBP': 20, 'VB': 12, ':': 12, 'RB': 8, 'IN': 7, 'CC': 7, '``': 6, 'POS': 5, 'VBD': 5, 'DT': 4, 'VBZ': 4, 'VBN': 3, 'PRP': 3, 'NNS': 2, 'CD': 2, 'WP': 2, \"''\": 2, 'MD': 1, 'RP': 1, 'JJR': 1}), Counter({'NN': 50, 'JJ': 20, 'POS': 6, 'VBZ': 4, 'VBP': 4, 'PRP': 2, 'IN': 2, 'VBD': 2, 'WDT': 1, 'RB': 1, 'FW': 1, 'NNS': 1, 'CD': 1, 'JJS': 1, 'CC': 1}), Counter({'NN': 26, 'JJ': 14, 'VBD': 4, 'RB': 4, 'VBP': 2, 'PRP': 2, 'POS': 1, 'MD': 1, 'VB': 1, 'WP$': 1, 'IN': 1, '``': 1, \"''\": 1, 'CD': 1}), Counter({'NN': 16, 'JJ': 6, 'VBP': 3, 'VB': 3, 'RB': 2, 'VBN': 1, 'DT': 1, 'VBD': 1, 'MD': 1}), Counter({'NN': 41, 'JJ': 22, 'RB': 11, 'VBP': 7, 'POS': 6, 'VBZ': 5, 'CC': 4, 'DT': 4, 'VBD': 3, 'NNS': 3, 'WP': 2, 'PRP': 2, 'IN': 2, 'CD': 1, 'MD': 1, 'VB': 1, ':': 1, '``': 1, \"''\": 1, 'VBN': 1}), Counter({'NN': 37, 'JJ': 8, 'VBP': 5, 'RB': 4, 'DT': 3, 'VBD': 3, 'POS': 3, 'VB': 3, 'PRP': 1, 'JJR': 1, 'IN': 1, '``': 1, \"''\": 1, 'VBZ': 1, 'NNS': 1, 'MD': 1}), Counter({'NN': 68, 'JJ': 27, 'VBD': 13, 'VB': 6, 'PRP': 5, 'RB': 5, 'VBP': 3, 'MD': 3, 'CD': 3, 'VBZ': 2, 'RBR': 2, 'NNS': 2, 'JJS': 1, 'VBN': 1, 'WP': 1, 'IN': 1, 'DT': 1, 'RP': 1}), Counter({'NN': 30, 'JJ': 13, 'DT': 3, 'RB': 3, 'IN': 3, 'VBP': 3, 'VBD': 2, 'VBN': 1, 'CD': 1, 'MD': 1, 'VB': 1}), Counter({'NN': 10, 'DT': 1, 'JJ': 1, 'IN': 1}), Counter({'NN': 28, 'JJ': 7, 'PRP': 2, 'RB': 2, 'POS': 2, 'VBP': 1, 'VBD': 1, 'NNS': 1, 'DT': 1, 'IN': 1, 'JJS': 1, 'VB': 1})]\n",
            "The total verbs: 403\n",
            "The total nouns: 305\n",
            "The total adjectives: 233\n",
            "The total adverbs: 156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n1WPBAeWPd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (2)Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences.\n",
        "import spacy\n",
        "!pip install benepar\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWrHbG8MAxMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "a1d94d1d-faf7-4e1a-cf5b-f8f007a20529"
      },
      "source": [
        "# (2)Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences.\n",
        "# Download and import necessarry packages\n",
        "import spacy\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "!pip install benepar\n",
        "from nltk.tree import Tree \n",
        "benepar.download('benepar_en2')\n",
        "nltk.download('punkt')\n",
        "import benepar\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Requirement already satisfied: benepar in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.6/dist-packages (from benepar) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from benepar) (1.17.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from benepar) (0.29.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2->benepar) (1.12.0)\n",
            "[nltk_data] Downloading package benepar_en2 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en2 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h04M3nZtK4pu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "042c57e7-90fd-48c2-c93d-be484a0293a1"
      },
      "source": [
        "\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: benepar in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from benepar) (1.17.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from benepar) (0.29.15)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.6/dist-packages (from benepar) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2->benepar) (1.12.0)\n",
            "(S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n",
            "('S',)\n",
            "The time for action\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d7CF7zzPwZm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "dc41d8aa-a7d7-4c91-bc33-10997a224632"
      },
      "source": [
        "from benepar.spacy_plugin import BeneparComponent\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "nlp.add_pipe(BeneparComponent('benepar_en'))\n",
        "doc = nlp(\"The time for action is now. It's never too late to do something.\")\n",
        "sent = list(doc.sents)[0]\n",
        "parse_sent = sent._.parse_string\n",
        "# (S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n",
        "print(sent._.labels)\n",
        "# ('S',)\n",
        "print(list(sent._.children)[0])\n",
        "# The time for action\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('S',)\n",
            "The time for action\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8x5DpqDSl79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "94405819-ac7e-4064-f352-cffbeb601268"
      },
      "source": [
        "#Constituency Parsing\n",
        "from nltk.tree import Tree \n",
        "benepar.download('benepar_en2')\n",
        "nltk.download('punkt')\n",
        "import benepar\n",
        "parser = benepar.Parser(\"benepar_en2\")\n",
        "tree = parser.parse(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
        "tree.pretty_print()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package benepar_en2 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en2 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "                              S                                          \n",
            "   ___________________________|________________________________________   \n",
            "  |          VP                                                        | \n",
            "  |     _____|__________                                               |  \n",
            "  |    |                VP                                             | \n",
            "  |    |      __________|________________                              |  \n",
            "  |    |     |                           PP                            | \n",
            "  |    |     |      _____________________|_____                        |  \n",
            "  |    |     |     |                           S                       | \n",
            "  |    |     |     |                           |                       |  \n",
            "  |    |     |     |                           VP                      | \n",
            "  |    |     |     |     ______________________|_______                |  \n",
            "  |    |     |     |    |          |                   PP              | \n",
            "  |    |     |     |    |          |            _______|___            |  \n",
            "  |    |     |     |    |          |           |           NP          | \n",
            "  |    |     |     |    |          |           |           |           |  \n",
            "  NP   |     |     |    |          NP          |           QP          | \n",
            "  |    |     |     |    |      ____|_____      |    _______|_____      |  \n",
            " NNP  VBZ   VBG    IN  VBG   NNP         NN    IN  $       CD    CD    . \n",
            "  |    |     |     |    |     |          |     |   |       |     |     |  \n",
            "Apple  is looking  at buying U.K.     startup for  $       1  billion  . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EakiWZ1uEzOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "a7e77cfa-080c-4831-c12b-10925077a161"
      },
      "source": [
        "#dependence parsing tree\n",
        "from spacy import displacy\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
        "displacy.render(doc, style = \"dep\", jupyter=True, options={\"distance\":140})"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
            "is be VERB VBZ aux xx True True\n",
            "looking look VERB VBG ROOT xxxx True False\n",
            "at at ADP IN prep xx True True\n",
            "buying buy VERB VBG pcomp xxxx True False\n",
            "U.K. U.K. PROPN NNP compound X.X. False False\n",
            "startup startup NOUN NN dobj xxxx True False\n",
            "for for ADP IN prep xxx True True\n",
            "$ $ SYM $ quantmod $ False False\n",
            "1 1 NUM CD compound d False False\n",
            "billion billion NUM CD pobj xxxx True False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"93ba066304574d6c9c1d3f7b4464848c-0\" class=\"displacy\" width=\"1590\" height=\"347.0\" direction=\"ltr\" style=\"max-width: none; height: 347.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"190\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"190\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"330\">looking</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"330\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"470\">at</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"470\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">buying</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">U.K.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">startup</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1030\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1030\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1170\">$</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1170\">SYM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">1</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"257.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">billion</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-0\" stroke-width=\"2px\" d=\"M70,212.0 C70,72.0 325.0,72.0 325.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,214.0 L62,202.0 78,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-1\" stroke-width=\"2px\" d=\"M210,212.0 C210,142.0 320.0,142.0 320.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M210,214.0 L202,202.0 218,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-2\" stroke-width=\"2px\" d=\"M350,212.0 C350,142.0 460.0,142.0 460.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M460.0,214.0 L468.0,202.0 452.0,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-3\" stroke-width=\"2px\" d=\"M490,212.0 C490,142.0 600.0,142.0 600.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M600.0,214.0 L608.0,202.0 592.0,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-4\" stroke-width=\"2px\" d=\"M770,212.0 C770,142.0 880.0,142.0 880.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,214.0 L762,202.0 778,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-5\" stroke-width=\"2px\" d=\"M630,212.0 C630,72.0 885.0,72.0 885.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M885.0,214.0 L893.0,202.0 877.0,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-6\" stroke-width=\"2px\" d=\"M630,212.0 C630,2.0 1030.0,2.0 1030.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1030.0,214.0 L1038.0,202.0 1022.0,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-7\" stroke-width=\"2px\" d=\"M1190,212.0 C1190,72.0 1445.0,72.0 1445.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1190,214.0 L1182,202.0 1198,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-8\" stroke-width=\"2px\" d=\"M1330,212.0 C1330,142.0 1440.0,142.0 1440.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1330,214.0 L1322,202.0 1338,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-93ba066304574d6c9c1d3f7b4464848c-0-9\" stroke-width=\"2px\" d=\"M1050,212.0 C1050,2.0 1450.0,2.0 1450.0,212.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-93ba066304574d6c9c1d3f7b4464848c-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1450.0,214.0 L1458.0,202.0 1442.0,202.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Y8MYX-JiBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "1ca4c04f-7d41-4932-edf7-472ab619b305"
      },
      "source": [
        "import spacy\n",
        "from nltk import Tree\n",
        "\n",
        "\n",
        "en_nlp = spacy.load('en')\n",
        "\n",
        "doc = en_nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
        "\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "\n",
        "\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          looking                           \n",
            "   __________|______________                 \n",
            "  |    |     |              at              \n",
            "  |    |     |              |                \n",
            "  |    |     |            buying            \n",
            "  |    |     |        ______|_______         \n",
            "  |    |     |       |             for      \n",
            "  |    |     |       |              |        \n",
            "  |    |     |    startup        billion    \n",
            "  |    |     |       |       _______|_____   \n",
            "Apple  is    .      U.K.    $             1 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSxui57sEexz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6f239fb6-bfa5-4561-c0d4-de1accdfdb0d"
      },
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "   print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz2YDabs5WPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "878c22ab-5edc-4446-fae2-9934bf2553a6"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-ca9956084537>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTja08-NtS6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "c9752674-0670-4349-825c-99eba408bbdd"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-b44d9ff70431>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pehK4ACtivx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "outputId": "2c24271b-8363-4fc1-a235-668e9702a15b"
      },
      "source": [
        " import stanfordnlp\n",
        "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
        "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
        "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
        "doc.sentences[0].print_dependencies()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 235M/235M [00:10<00:00, 19.6MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "('Barack', '4', 'nsubj:pass')\n",
            "('Obama', '1', 'flat')\n",
            "('was', '4', 'aux:pass')\n",
            "('born', '0', 'root')\n",
            "('in', '6', 'case')\n",
            "('Hawaii', '4', 'obl')\n",
            "('.', '4', 'punct')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQdkRw5QuuJd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "89d56b62-eef7-45a2-842b-f6086de1b26b"
      },
      "source": [
        "!pip install pycorenlp"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycorenlp\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/40/e74eb4fc7906d630b73a84c9ae9d824f694bd4c5a1d727b8e18beadff613/pycorenlp-0.3.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pycorenlp) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp) (3.0.4)\n",
            "Building wheels for collected packages: pycorenlp\n",
            "  Building wheel for pycorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycorenlp: filename=pycorenlp-0.3.0-cp36-none-any.whl size=2143 sha256=96bf71a93af31cb4c4308d5b89c812b37a3b4c1f55666c67b0ff8db694b37b8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/e9/2f/767a7b5f2e82d587a36143c04a21839b4b14bebfb89410d2d5\n",
            "Successfully built pycorenlp\n",
            "Installing collected packages: pycorenlp\n",
            "Successfully installed pycorenlp-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv8k-xkLxY6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pycorenlp import StanfordCoreNLP\n",
        "nlp = StanfordCoreNLP('http://localhost:9000')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POiKoSgfxdxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.parse import CoreNLPParser\n",
        "\n",
        "result = CoreNLPParser(url='http://localhost:9000')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gztbUcIx31Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "nlp_wrapper = StanfordCoreNLP('http://localhost:9000')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFnFWLwa3mGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "1001e7e1-130b-4cef-f4e9-cdca73b377d3"
      },
      "source": [
        "doc = \"Ronaldo has moved from Real Madrid to Juventus. While messi still plays for Barcelona\"\n",
        "nlp_wrapper = result.annotate(doc,\n",
        "    properties={\n",
        "        'annotators': 'ner, pos',\n",
        "        'outputFormat': 'json',\n",
        "        'timeout': 1000,\n",
        "    })"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-999e88f29d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Ronaldo has moved from Real Madrid to Juventus. While messi still plays for Barcelona\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m nlp_wrapper = result.annotate(doc,\n\u001b[0m\u001b[1;32m      3\u001b[0m     properties={\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ner, pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CoreNLPParser' object has no attribute 'annotate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vxzu0mth02_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "7e537014-004b-4ff2-cc89-b2d2c203c718"
      },
      "source": [
        "#constitutuent Parsing with Standford NP\n",
        "import nltk\n",
        "from nltk.parse.stanford import StanfordParser\n",
        "def traverse_phrase(tree): \n",
        "\n",
        "    for subtree in tree:\n",
        "\n",
        "        if type(subtree) == nltk.tree.Tree:\n",
        "\n",
        "            traverse_phrase(subtree)\n",
        "\n",
        "        else:\n",
        "\n",
        "            print( \"constituent : \" + subtree )\n",
        "\n",
        "def traverse_tree(tree):\n",
        "\n",
        "    for subtree in tree:\n",
        "\n",
        "        if type(subtree) == nltk.tree.Tree:\n",
        "\n",
        "            if subtree.label() == 'NP':\n",
        "\n",
        "                print(\"\\n[Noun Phrase]\")\n",
        "\n",
        "                traverse_phrase(subtree)\n",
        "\n",
        "            else :\n",
        "\n",
        "                traverse_tree(subtree)\n",
        "\n",
        "stanford_parser_dir = \"C:\\\\Users\\ngoch\\\\stanford-parser-full-2015-04-20\\\\\" \n",
        "path_to_models = stanford_parser_dir  + \"stanford-parser-3.5.2-models.jar\"\n",
        "path_to_jar = stanford_parser_dir  + \"stanford-parser.jar\"\n",
        "sentence = (\"The cute cat chases the mouse\")\n",
        "\n",
        "# Phrase structure parser\n",
        "\n",
        "parser=StanfordParser(path_to_models, path_to_jar)\n",
        "\n",
        "print( \"\\n\\nPhrase Structure Parsing Result - will only print NPs (Noun Phrases)\")\n",
        "parsed_sentence = parser.raw_parse(sentence)\n",
        "traverse_tree(parsed_sentence)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-265c046de79d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Phrase structure parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStanfordParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"\\n\\nPhrase Structure Parsing Result - will only print NPs (Noun Phrases)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at C:\\Users\ngoch\\stanford-parser-full-2015-04-20\\stanford-parser-3.5.2-models.jar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXyVhY0iJOE2",
        "colab_type": "code",
        "outputId": "ad41fb1a-ee7b-4c98-dfe9-8ebc6982fecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
        "sentence_nlp = [\"Spacy had two types of English dependency parsers based on what language models you use\"]\n",
        "for token in sentence_nlp:\n",
        "    print(dependency_pattern.format(word=token.orth_, \n",
        "                                  w_type=token.dep_,\n",
        "                                  left=[t.orth_ \n",
        "                                            for t \n",
        "                                            in token.lefts],\n",
        "                                  right=[t.orth_ \n",
        "                                             for t \n",
        "                                             in token.rights]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d89ae7d9bf3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Spacy had two types of English dependency parsers based on what language models you use\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_nlp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     print(dependency_pattern.format(word=token.orth_, \n\u001b[0m\u001b[1;32m      5\u001b[0m                                   \u001b[0mw_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   left=[t.orth_ \n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'orth_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3I52HpLtP9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y7Iohm4J9e0",
        "colab_type": "code",
        "outputId": "18943f35-6c73-4f74-bf97-10aed948eba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "sentence = str(news_df.iloc[1].full_text)\n",
        "sentence_nlp = nlp(sentence)\n",
        "\n",
        "# print named entities in article\n",
        "print([(word, word.ent_type_) for word in sentence_nlp if word.ent_type_])\n",
        "\n",
        "# visualize named entities\n",
        "displacy.render(sentence_nlp, style='ent', jupyter=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-87e05ea5ccda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentence_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print named entities in article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_nlp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ment_type_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxtqmsG8Df08",
        "colab_type": "code",
        "outputId": "9dbdada0-e73d-440b-ba31-2b295387fa40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "import os\n",
        "print(\"JAVA_HOME:\", os.environ['JAVA_HOME']) \n",
        "java_path = r'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "\n",
        "from nltk.parse.stanford import StanfordParser\n",
        "scp = StanfordParser(path_to_jar= r\"C:\\Users\\ngoch\\stanford-parser-full-2015-04-20\\stanford-parser.jar\",\n",
        "                     path_to_models_jar= r\"C:\\Users\\ngoch\\stanford-parser-full-2015-04-20\\stanford-parser-3.5.2-models.jar\")\n",
        "result = list(scp.raw_parse(\"I saw the joker last night\"))\n",
        "print(result[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f101e0500b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JAVA_HOME:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JAVA_HOME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mjava_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'C:\\Program Files\\Java\\jdk1.8.0_102\\bin\\java.exe'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JAVAHOME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjava_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'JAVA_HOME'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js_SPmnYXmMe",
        "colab_type": "code",
        "outputId": "7ffdb427-4cca-4b39-9981-3df0ee4dae5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "source": [
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  VP -> V NP | V NP PP\n",
        "  PP -> P NP\n",
        "  V -> \"saw\" | \"ate\" | \"walked\"\n",
        "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
        "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
        "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
        "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
        "  \"\"\")\n",
        "print(grammar1)\n",
        "for sent in cleaned_content:\n",
        "  rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        "  for tree in rd_parser.parse(sent):\n",
        "     print(tree)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grammar with 25 productions (start state = S)\n",
            "    S -> NP VP\n",
            "    VP -> V NP\n",
            "    VP -> V NP PP\n",
            "    PP -> P NP\n",
            "    V -> 'saw'\n",
            "    V -> 'ate'\n",
            "    V -> 'walked'\n",
            "    NP -> 'John'\n",
            "    NP -> 'Mary'\n",
            "    NP -> 'Bob'\n",
            "    NP -> Det N\n",
            "    NP -> Det N PP\n",
            "    Det -> 'a'\n",
            "    Det -> 'an'\n",
            "    Det -> 'the'\n",
            "    Det -> 'my'\n",
            "    N -> 'man'\n",
            "    N -> 'dog'\n",
            "    N -> 'cat'\n",
            "    N -> 'telescope'\n",
            "    N -> 'park'\n",
            "    P -> 'in'\n",
            "    P -> 'on'\n",
            "    P -> 'by'\n",
            "    P -> 'with'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dd81f27c92a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_content\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mrd_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveDescentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/recursivedescent.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Start a recursive descent parse, with an initial tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[0;32m--> 648\u001b[0;31m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: '\\'i\\', \\'person\\', \\'hype\\', \\'claim\\', \\'masterpiec\\', \\'overreact\\', \\'overblown\\', \\'excit\\', \\'anoth\\', \\'joker\\', \\'base\\', \\'film\\', \\'i\\', \\'thought\\', \\'look\\', \\'solid\\', \\'best\\', \\'even\\', \\'bit\\', \\'pretenti\\', \\'trailer\\', \\'say\\', \\'i\\', \\'incred\\', \\'wrong\\', \\'thi\\', \\'massiv\\', \\'achiev\\', \\'cinema\\', \"\\'s\", \\'extrem\\', \\'rare\\', \\'day\\', \\'age\\', \\'cgi\\', \\'nonsens\\', \\'reboot\\', \\'while\\', \\'somewhat\\', \\'reboot\\', \\'sort\\', \\'standalon\\', \\'origin\\', \\'tale\\', \\'impecc\\', \\'start\\', \\'finish\\', \\'echo\\', \\'resembl\\', \\'best\\', \\'joker\\', \\'origin\\', \\'comic\\', \\'past\\', \\'joaquin\\', \\'bleed\\', \\'sweat\\', \\'cri\\', \\'everi\\', \\'drop\\', \\'magnific\\', \\'dedic\\', \\'perform\\', \\'heath\\', \\'ledger\\', \\'would\\', \\'proud\\', \\'thi\\', \\'undoubtedli\\', \\'greatest\\', \\'act\\', \\'perform\\', \\'sinc\\', \\'heath\\', \"\\'s\", \\'joker\\', \\'direct\\', \\'write\\', \\'slickli\\', \\'brilliant\\', \\'bleak\\', \\'set\\', \\'tone\\', \\'palpabl\\', \\'throughout\\', \\'when\\', \\'film\\', \\'place\\', \\'blown\\', \\'away\\', \\'everi\\', \\'audienc\\', \\'member\\', \\'awestruck\\', \\'wit\\', \\'film\\', \\'could\\', \\'still\\', \\'transport\\', \\'charact\\', \"\\'s\", \\'world\\', \\'exist\\', \\'believ\\', \\'hype\\', \\'thi\\', \\'go\\', \\'rever\\', \\'transcend\\', \\'masterpiec\\', \\'cinema\\''."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    }
  ]
}