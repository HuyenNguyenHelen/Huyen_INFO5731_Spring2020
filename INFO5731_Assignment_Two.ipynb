{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/Huyen_INFO5731_Spring2020/blob/master/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzMjKmiPTHlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "bd9b4f83-788b-49c3-ddb3-ecc255744472"
      },
      "source": [
        "# Question1\n",
        "\n",
        "#Yesss I fixed that problem\n",
        "\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "host_url = [\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"]\n",
        "load_more_urls = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "all_urls = host_url+load_more_urls\n",
        "user_names, user_links, content, date, title, review_helpfulness = [],[], [], [], [], []\n",
        "review_elements = []\n",
        "for url in all_urls:\n",
        "  request = urllib.request.urlopen (url)\n",
        "  webpage  = request.read()\n",
        "  soup = BeautifulSoup(webpage)\n",
        "\n",
        "  reviews = soup.find_all(\"div\", {\"class\":\"lister-item-content\" })\n",
        "  for review in reviews:\n",
        "    try:\n",
        "        review_title = review.find(\"a\", class_=\"title\").text.strip()\n",
        "    except:\n",
        "        review_title = \"\"\n",
        "    try:\n",
        "        review_id = review.find(\"a\")[\"href\"].split(\"/\")[2]\n",
        "    except:\n",
        "        review_id = \"\"\n",
        "    try:\n",
        "        review_date = review.find(\"span\", class_=\"review-date\").get_text()\n",
        "    except:\n",
        "        review_date = \"\"\n",
        "    try:\n",
        "        review_nickname = review.find (\"span\", class_=\"display-name-link\").get_text()\n",
        "    except:\n",
        "        review_nickname = \"\"\n",
        "    try:\n",
        "        review_content = review.find (\"div\", class_=\"text show-more__control\").get_text()\n",
        "    except:\n",
        "        review_content =\"\"\n",
        "    #print(review_content)\n",
        "    try:\n",
        "        review_rating = review. find (\"span\", class_=\"rating-other-user-rating\").getText().strip()\n",
        "        #print(review_rating)\n",
        "    except:\n",
        "        review_rating = \"\"\n",
        "    try:\n",
        "        review_helpfulness = review.find(\"div\", {\"class\": \"actions text-muted\"}).text.split(\"\\n\")[1]\n",
        "        #print(review_helpfulness)\n",
        "    except:\n",
        "        review_helpfulness = \"\"\n",
        "\n",
        "    review_elements.append([review_title, review_id, review_date, review_nickname, review_content, review_rating, review_helpfulness])\n",
        "\n",
        "df = pd.DataFrame(review_elements, columns=[\"review_title\", \"review_id\", \"review_date\", \"review_nickname\", \"review_content\", \"review_rating\", \"review_helpfulness\"])\n",
        "df\n",
        "\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"w\", newline=\"\", encoding='utf-8') as file:\n",
        "  df.to_csv(file)\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"r\", encoding='utf-8') as file:\n",
        "  pd.read_csv(file)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_title</th>\n",
              "      <th>review_id</th>\n",
              "      <th>review_date</th>\n",
              "      <th>review_nickname</th>\n",
              "      <th>review_content</th>\n",
              "      <th>review_rating</th>\n",
              "      <th>review_helpfulness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As a viewer that actually went to TIFF and wit...</td>\n",
              "      <td>rw5112402</td>\n",
              "      <td>10 September 2019</td>\n",
              "      <td>JF500</td>\n",
              "      <td>I was a person that saw all the hype and claim...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>6,833 out of 7,853 found t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outstanding movie with a haunting performance ...</td>\n",
              "      <td>rw5159304</td>\n",
              "      <td>3 October 2019</td>\n",
              "      <td>MihaVrhunc</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>3,374 out of 4,038 found t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Only certain people can relate</td>\n",
              "      <td>rw5168360</td>\n",
              "      <td>7 October 2019</td>\n",
              "      <td>lesterarnoldpinto</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>2,672 out of 3,298 found t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Must have put a SMILE of Satisfaction on Heath...</td>\n",
              "      <td>rw5092831</td>\n",
              "      <td>1 September 2019</td>\n",
              "      <td>Chandler_Bing_</td>\n",
              "      <td>Truly a masterpiece, The Best film of 2019, on...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>3,165 out of 3,932 found t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Hype is real</td>\n",
              "      <td>rw5160204</td>\n",
              "      <td>4 October 2019</td>\n",
              "      <td>kdagoulis26</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>2,351 out of 2,917 found t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>So disappointed by this film</td>\n",
              "      <td>rw5187643</td>\n",
              "      <td>14 October 2019</td>\n",
              "      <td>huntj-59716</td>\n",
              "      <td>Was really looking forward to seeing this film...</td>\n",
              "      <td>2/10</td>\n",
              "      <td>71 out of 139 found this h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Well filmed but empty</td>\n",
              "      <td>rw5179068</td>\n",
              "      <td>11 October 2019</td>\n",
              "      <td>Red_Identity</td>\n",
              "      <td>What can I say about it that hasn't already be...</td>\n",
              "      <td></td>\n",
              "      <td>41 out of 76 found this he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Spare me the \"best comic book movie ever\" nons...</td>\n",
              "      <td>rw5144576</td>\n",
              "      <td>26 September 2019</td>\n",
              "      <td>gatozangado</td>\n",
              "      <td>It premiered last night in Sydney and I was lu...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>102 out of 206 found this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>As movie well overrated - acting excellent</td>\n",
              "      <td>rw5175282</td>\n",
              "      <td>9 October 2019</td>\n",
              "      <td>dhllon</td>\n",
              "      <td>The depth of the story was lacking to much foc...</td>\n",
              "      <td>2/10</td>\n",
              "      <td>59 out of 114 found this h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>Lives up to the hype</td>\n",
              "      <td>rw5158293</td>\n",
              "      <td>3 October 2019</td>\n",
              "      <td>i_like_jellycups</td>\n",
              "      <td>A breath of fresh airThink heath ledgers joker...</td>\n",
              "      <td>10/10</td>\n",
              "      <td>99 out of 200 found this h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>115 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          review_title  ...                                 review_helpfulness\n",
              "0    As a viewer that actually went to TIFF and wit...  ...                      6,833 out of 7,853 found t...\n",
              "1    Outstanding movie with a haunting performance ...  ...                      3,374 out of 4,038 found t...\n",
              "2                       Only certain people can relate  ...                      2,672 out of 3,298 found t...\n",
              "3    Must have put a SMILE of Satisfaction on Heath...  ...                      3,165 out of 3,932 found t...\n",
              "4                                     The Hype is real  ...                      2,351 out of 2,917 found t...\n",
              "..                                                 ...  ...                                                ...\n",
              "110                       So disappointed by this film  ...                      71 out of 139 found this h...\n",
              "111                              Well filmed but empty  ...                      41 out of 76 found this he...\n",
              "112  Spare me the \"best comic book movie ever\" nons...  ...                      102 out of 206 found this ...\n",
              "113         As movie well overrated - acting excellent  ...                      59 out of 114 found this h...\n",
              "114                               Lives up to the hype  ...                      99 out of 200 found this h...\n",
              "\n",
              "[115 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6v-2mpvil47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "dc84d254-d3ba-498a-c268-982cc271e4c3"
      },
      "source": [
        "#try_codes\n",
        "load_more_data = soup.find_all(\"div\", attrs={\"class\": \"load-more-data\"}) # find this button at the end of each page\n",
        "print(load_more_data)\n",
        "for item in load_more_data:\n",
        "  path_parse1 = \"https://www.imdb.com\"\n",
        "  path_parse2 = \"/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=\"\n",
        "  path_parse3 = item.get(\"data-key\")\n",
        "  concatenated_path = path_parse1+path_parse2+path_parse3\n",
        "  print(concatenated_path)\n",
        "#load_more_links = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "#for link in load_more_links:\n",
        " # print(link) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<div class=\"load-more-data\" data-key=\"g4wp7cboqa3de3yh62xxxmzzrlr4sazhzfmxvlnomwklyczuf43o6ss6oe2flnbddz4k5npu46r6k33mufppgdsl6gqg4di\">\n",
            "</div>]\n",
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cboqa3de3yh62xxxmzzrlr4sazhzfmxvlnomwklyczuf43o6ss6oe2flnbddz4k5npu46r6k33mufppgdsl6gqg4di\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBytY15j3Hkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "50c0e461-d9da-45c4-82b9-c056744f35a7"
      },
      "source": [
        "host_url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "\n",
        "def link_load_more (host_url):\n",
        "  request = urllib.request.urlopen (host_url)\n",
        "  webpage  = request.read()\n",
        "  soup = BeautifulSoup(webpage)\n",
        "  load_more_data = soup.find(\"div\", attrs={\"class\": \"load-more-data\"}) # find this button at the end of each page\n",
        "  print(load_more_data)\n",
        "  for item in load_more_data:\n",
        "    path_parse1 = \"https://www.imdb.com/\"\n",
        "    path_parse2 = \"title/tt7286456/reviews/_ajax\"\n",
        "    path_parse3 = \"?ref_=undefined&paginationKey=\"\n",
        "    path_parse4 = item.get(\"data-key\")\n",
        "    concatenated_path = path_parse1+path_parse2+path_parse3+path_parse4\n",
        "  return concatenated_path\n",
        "\n",
        "\n",
        "\n",
        "# generate link and parse all of four pages\n",
        "#host_url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "#print(type(host_url))\n",
        "\n",
        "url_nextpage1 = link_load_more(host_url)\n",
        "#url_nextpage2 = link_load_more(url_nextpage1)\n",
        "#url_nextpage3 = link_load_more(url_nextpage2)\n",
        "#url_nextpage4 = link_load_more(url_nextpage3)\n",
        "\n",
        "#print(url_nextpage1)\n",
        "#print(url_nextpage2)\n",
        "#print(url_nextpage3)\n",
        "#print(url_nextpage4)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<div class=\"load-more-data\" data-ajaxurl=\"/title/tt7286456/reviews/_ajax\" data-key=\"g4wp7crmqu2ta3qb7oux7nrvqpr42brhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k4rmwb65sqjd2c5p4ic5kj7njmla\">\n",
            "<div class=\"ipl-load-more ipl-load-more--loaded\">\n",
            "<div class=\"ipl-load-more__load-indicator\">\n",
            "<svg class=\"ipl-dot-loader\" height=\"7px\" version=\"1.1\" viewbox=\"0 0 37 7\" width=\"37px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
            "<g class=\"ipl-dot-loader__container\" fill=\"#000000\">\n",
            "<circle class=\"ipl-dot-loader__dot ipl-dot-loader__dot--one\" cx=\"3.5\" cy=\"3.5\" r=\"3.5\"></circle>\n",
            "<circle class=\"ipl-dot-loader__dot ipl-dot-loader__dot--two\" cx=\"18.5\" cy=\"3.5\" r=\"3.5\"></circle>\n",
            "<circle class=\"ipl-dot-loader__dot ipl-dot-loader__dot--three\" cx=\"33.5\" cy=\"3.5\" r=\"3.5\"></circle>\n",
            "</g>\n",
            "</svg>\n",
            "</div>\n",
            "<div class=\"ipl-message-box ipl-message-box--alert ipl-load-more__message-box ipl-load-more__message-box--hidden\">\n",
            "<svg class=\"ipl-icon ipl-alert-icon \" fill=\"#000000\" height=\"24\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "<path d=\"M0 0h24v24H0z\" fill=\"none\"></path>\n",
            "<path d=\"M1 21h22L12 2 1 21zm12-3h-2v-2h2v2zm0-4h-2v-4h2v4z\"></path>\n",
            "</svg>\n",
            "<span class=\"ipl-load-more__error-text\">An error has occured. Please try again.</span>\n",
            "</div>\n",
            "<button class=\"ipl-load-more__button\" data-target-container=\"reviews-container\" id=\"load-more-trigger\">Load More</button>\n",
            "</div>\n",
            "</div>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-55588619c9fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#print(type(host_url))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0murl_nextpage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#url_nextpage2 = link_load_more(url_nextpage1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#url_nextpage3 = link_load_more(url_nextpage2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-55588619c9fb>\u001b[0m in \u001b[0;36mlink_load_more\u001b[0;34m(host_url)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpath_parse2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"title/tt7286456/reviews/_ajax\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpath_parse3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"?ref_=undefined&paginationKey=\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpath_parse4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data-key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mconcatenated_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_parse1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath_parse2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath_parse3\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath_parse4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenated_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    795\u001b[0m             raise AttributeError(\n\u001b[1;32m    796\u001b[0m                 \"'%s' object has no attribute '%s'\" % (\n\u001b[0;32m--> 797\u001b[0;31m                     self.__class__.__name__, attr))\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moutput_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve4dQ5PPuexW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0531edfd-7291-47e2-8b73-6cad256158f8"
      },
      "source": [
        "for review in reviews:\n",
        "    try:\n",
        "      load_more_urls= review.find(\"div\", attrs={\"class\": \"load-more-data\"})\n",
        "    except:\n",
        "      load_more_urls = \"None\"\n",
        "print(load_more_urls)\n",
        " "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IshZWGtzPJrj",
        "colab_type": "code",
        "outputId": "b65fc670-7eca-4d59-e2e8-4ceeede8e321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "host = [\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"]\n",
        "load_more_links = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "urls = host+load_more_links\n",
        "print(len(urls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPdP0QkuNZKm",
        "colab_type": "code",
        "outputId": "afc55f73-a8be-4aaf-ccd2-b3d2a6e3390a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from urllib.parse import urlparse\n",
        "urlparse(\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParseResult(scheme='https', netloc='www.imdb.com', path='/title/tt7286456/reviews', params='', query='ref_=tt_urv', fragment='')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "#clean the text data\n",
        "from __future__ import division\n",
        "#import nltk, re, pprint\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string \n",
        "\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"r\", encoding='utf-8') as file:\n",
        "  data = pd.read_csv(file)\n",
        "\n",
        "raw_title = [str(line) for line in data.review_title]\n",
        "raw_content = [str(line) for line in data.review_content]\n",
        "# Tokenize raw text in raw_title and raw_content\n",
        "tokens_title= [nltk.word_tokenize(title) for title in raw_title]\n",
        "tokens_content = [nltk.word_tokenize(item) for item in raw_content]\n",
        "\n",
        "# PROCCESSES OF CLEANING TEXT  \n",
        "def cleaning_text (text):\n",
        "    step1 = remove_noise(text)\n",
        "    step2 = remove_num (step1)\n",
        "    step3 = remove_stop (step2)\n",
        "    step4 = lowercase (step3)\n",
        "    step5 = stemming (step4)\n",
        "    step6 = lemma (step5)\n",
        "    return step6\n",
        "\n",
        "# (1) Remove noise, such as special characters and punctuations.      \n",
        "def remove_noise (list):\n",
        "    punct=[char for char in string.punctuation]\n",
        "    noise_removed = [word for word in list if word not in punct]\n",
        "    return noise_removed\n",
        "\n",
        "#(2) Remove numbers.\n",
        "def remove_num (list):\n",
        "    num_removed  = [word for word in list if not word.isnumeric()]\n",
        "    return num_removed\n",
        "\n",
        " # (3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "def remove_stop (list):\n",
        "    stopword_removed = [word for word in list if word not in stopwords.words('english')]\n",
        "    return stopword_removed\n",
        "\n",
        "#(4) Lowercase all texts\n",
        "def lowercase (list):\n",
        "    lowercase = [word.lower() for word in list]\n",
        "    return lowercase\n",
        "# (5) Stemming\n",
        "def stemming (list):\n",
        "    ps = PorterStemmer()\n",
        "    word_stemmed = [ps.stem(word) for word in list]\n",
        "    return word_stemmed\n",
        "# (6) Lemmatization.\n",
        "def lemma (list):\n",
        "    wnl =  WordNetLemmatizer ()\n",
        "    word_lemmatized = [wnl.lemmatize(word) for word in list]\n",
        "    return word_lemmatized\n",
        "\n",
        "# Cleaning title reviews and content review\n",
        "cleaned_title = [cleaning_text (sublist) for sublist in tokens_title]\n",
        "cleaned_content = [cleaning_text (sublist) for sublist in tokens_content]\n",
        "# Adding cleaned texts to new columns of dataframe\n",
        "dict.update ({\"cleaned_review_title\":cleaned_title, \"cleaned_review_content\": cleaned_content })\n",
        "df = pd.DataFrame(dict)\n",
        "df\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "#(1) Tag arts of Speech (POS) and  calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb)...\n",
        "\n",
        "from collections import Counter\n",
        "for tokens in cleaned_title:\n",
        "    title_tagged = nltk.pos_tag(tokens)\n",
        "    #fd_title = nltk.FreqDist(tag for (token, tag) in title_tagged)\n",
        "    #print(fd_title)\n",
        "    title_tagged_counts = Counter (tag for tokens, tag in title_tagged)\n",
        "    print(title_tagged_counts)\n",
        "\n",
        "for tokens in cleaned_content:\n",
        "    content_tagged = nltk.pos_tag(tokens)\n",
        "    #fd_title = nltk.FreqDist(tag for (token, tag) in title_tagged)\n",
        "    #print(fd_title)\n",
        "    content_tagged_counts = Counter (tag for tokens, tag in content_tagged)\n",
        "    print(content_tagged_counts)\n",
        "  \n",
        "# (2)Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    }
  ]
}