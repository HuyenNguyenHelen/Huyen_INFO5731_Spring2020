{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/Huyen_INFO5731_Spring2020/blob/master/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPjGGLvpVGE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzMjKmiPTHlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Question1\n",
        "\n",
        "#Yesss I fixed that problem\n",
        "\n",
        "host_url = [\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"]\n",
        "load_more_urls = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "all_urls = host_url+load_more_urls\n",
        "user_names, user_links, content, date, title, review_helpfulness = [],[], [], [], [], []\n",
        "review_elements = []\n",
        "for url in all_urls:\n",
        "  request = urllib.request.urlopen (url)\n",
        "  webpage  = request.read()\n",
        "  soup = BeautifulSoup(webpage)\n",
        "\n",
        "  reviews = soup.find_all(\"div\", {\"class\":\"lister-item-content\" })\n",
        "  for review in reviews:\n",
        "    try:\n",
        "        review_title = review.find(\"a\", class_=\"title\").text.strip()\n",
        "    except:\n",
        "        review_title = \"\"\n",
        "    try:\n",
        "        review_id = review.find(\"a\")[\"href\"].split(\"/\")[2]\n",
        "    except:\n",
        "        review_id = \"\"\n",
        "    try:\n",
        "        review_date = review.find(\"span\", class_=\"review-date\").get_text()\n",
        "    except:\n",
        "        review_date = \"\"\n",
        "    try:\n",
        "        review_nickname = review.find (\"span\", class_=\"display-name-link\").get_text()\n",
        "    except:\n",
        "        review_nickname = \"\"\n",
        "    try:\n",
        "        review_content = review.find (\"div\", class_=\"text show-more__control\").get_text()\n",
        "    except:\n",
        "        review_content =\"\"\n",
        "    #print(review_content)\n",
        "    try:\n",
        "        review_rating = review. find (\"span\", class_=\"rating-other-user-rating\").getText().strip()\n",
        "        #print(review_rating)\n",
        "    except:\n",
        "        review_rating = \"\"\n",
        "    try:\n",
        "        review_helpfulness = review.find(\"div\", {\"class\": \"actions text-muted\"}).text.split(\"\\n\")[1]\n",
        "        #print(review_helpfulness)\n",
        "    except:\n",
        "        review_helpfulness = \"\"\n",
        "\n",
        "    review_elements.append([review_title, review_id, review_date, review_nickname, review_content, review_rating, review_helpfulness])\n",
        "\n",
        "df = pd.DataFrame(review_elements, columns=[\"review_title\", \"review_id\", \"review_date\", \"review_nickname\", \"review_content\", \"review_rating\", \"review_helpfulness\"])\n",
        "df\n",
        "\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"w\", newline=\"\", encoding='utf-8') as file:\n",
        "  df.to_csv(file)\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"r\", encoding='utf-8') as file:\n",
        "  pd.read_csv(file)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6v-2mpvil47",
        "colab_type": "code",
        "outputId": "1ebdf538-938f-4b05-bc95-4380ead40255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#try_codes\n",
        "load_more_data = soup.find_all(\"div\", attrs={\"class\": \"load-more-data\"}) # find this button at the end of each page\n",
        "#print(load_more_data)\n",
        "for item in load_more_data:\n",
        "  path_parse1 = \"https://www.imdb.com\"\n",
        "  path_parse2 = \"/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=\"\n",
        "  path_parse3 = item.get(\"data-key\")\n",
        "  concatenated_path = path_parse1+path_parse2+path_parse3\n",
        "  print(concatenated_path)\n",
        "#load_more_links = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "#for link in load_more_links:\n",
        " # print(link) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqu2ta3qb7oux7nrvqpr42brhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k4rmwb65sqjd2c5p4ic5kj7njmla\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBytY15j3Hkx",
        "colab_type": "code",
        "outputId": "cf096a5f-7f72-4ab8-f204-64c3f0aa51b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "# In this line, I am attempting to generate links with data-key in the class load-more-data. However, because, the server prohibits to get key in the second next page, so my code raise error.\n",
        "host_url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "def link_load_more (host_url):\n",
        "  request = urllib.request.urlopen (host_url)\n",
        "  webpage  = request.read()\n",
        "  soup = BeautifulSoup(webpage)\n",
        "  load_more_data = soup.find_all(\"div\", attrs={\"class\": \"load-more-data\"}) # find this button at the end of each page\n",
        "  flag = True\n",
        "  if len (load_more_data):\n",
        "  #print(load_more_data)\n",
        "    for item in load_more_data:\n",
        "      path_parse1 = \"https://www.imdb.com/\"\n",
        "      path_parse2 = \"title/tt7286456/reviews/_ajax\"\n",
        "      path_parse3 = \"?ref_=undefined&paginationKey=\"\n",
        "      try:\n",
        "        path_parse4 = item.get(\"data-key\")\n",
        "        concatenated_path = path_parse1+path_parse2+path_parse3+path_parse4\n",
        "      except KeyError:\n",
        "        flag = False\n",
        "  return concatenated_path\n",
        "\n",
        "# generate link and parse all of four pages\n",
        "\n",
        "url_nextpage1 = link_load_more(host_url)\n",
        "url_nextpage2 = link_load_more(url_nextpage1)\n",
        "url_nextpage3 = link_load_more(url_nextpage2)\n",
        "url_nextpage4 = link_load_more(url_nextpage3)\n",
        "\n",
        "print(url_nextpage1)\n",
        "print(url_nextpage2)\n",
        "print(url_nextpage3)\n",
        "print(url_nextpage4)\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-79216aae5d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0murl_nextpage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0murl_nextpage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_nextpage1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0murl_nextpage3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_nextpage2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0murl_nextpage4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_load_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_nextpage3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-79216aae5d21>\u001b[0m in \u001b[0;36mlink_load_more\u001b[0;34m(host_url)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenated_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# generate link and parse all of four pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'concatenated_path' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve4dQ5PPuexW",
        "colab_type": "code",
        "outputId": "0531edfd-7291-47e2-8b73-6cad256158f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "for review in reviews:\n",
        "    try:\n",
        "      load_more_urls= review.find(\"div\", attrs={\"class\": \"load-more-data\"})\n",
        "    except:\n",
        "      load_more_urls = \"None\"\n",
        "print(load_more_urls)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IshZWGtzPJrj",
        "colab_type": "code",
        "outputId": "b65fc670-7eca-4d59-e2e8-4ceeede8e321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "host = [\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"]\n",
        "load_more_links = [\"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7crmqm2dczaa7wxhfmzzrdtmsbzhzfmxvlnomwklyczuf43o6ss6oe2fjmzldj4k5tm5fjyr3a2pn6woclsokk47cma\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cjmqqzde3yk7swhhmjqrxt4ohjjtzpwzouokkd2gbzgpnt6ucc2peyv5mbjb4dwdpdqzcxjfmg3k65z4tqdaz44w\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbkqmzdizyc76wxvnzrrxs46bzhzfmxvlnomwklyczuf43o6ss6oe4vtpridv4k5vz4e43vvsqoskmtj4psbfq2qni\", \"https://www.imdb.com/title/tt7286456/reviews/_ajax?ref_=undefined&paginationKey=g4wp7cbpryzd6yab62tx7obtrtt4objhzfmxvlnomwklyczuf43o6ss6oe4f5nzldn4k4k7d72vy22axhza7lis2c2y7jzi\"]\n",
        "urls = host+load_more_links\n",
        "print(len(urls))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPdP0QkuNZKm",
        "colab_type": "code",
        "outputId": "afc55f73-a8be-4aaf-ccd2-b3d2a6e3390a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from urllib.parse import urlparse\n",
        "urlparse(\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParseResult(scheme='https', netloc='www.imdb.com', path='/title/tt7286456/reviews', params='', query='ref_=tt_urv', fragment='')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "#clean the text data\n",
        "from __future__ import division\n",
        "#import nltk, re, pprint\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string \n",
        "\n",
        "with open(\"E:\\Helen\\INFO5731_Assignment2\\Joke_reviews.csv\", \"r\", encoding='utf-8') as file:\n",
        "  data = pd.read_csv(file)\n",
        "\n",
        "raw_title = [str(line) for line in data.review_title]\n",
        "raw_content = [str(line) for line in data.review_content]\n",
        "# Tokenize raw text in raw_title and raw_content\n",
        "tokens_title= [nltk.word_tokenize(title) for title in raw_title]\n",
        "tokens_content = [nltk.word_tokenize(item) for item in raw_content]\n",
        "\n",
        "# PROCCESSES OF CLEANING TEXT  \n",
        "def cleaning_text (text):\n",
        "    step1 = remove_noise(text)\n",
        "    step2 = remove_num (step1)\n",
        "    step3 = remove_stop (step2)\n",
        "    step4 = lowercase (step3)\n",
        "    step5 = stemming (step4)\n",
        "    step6 = lemma (step5)\n",
        "    return step6\n",
        "\n",
        "# (1) Remove noise, such as special characters and punctuations.      \n",
        "def remove_noise (list):\n",
        "    punct=[char for char in string.punctuation]\n",
        "    noise_removed = [word for word in list if word not in punct]\n",
        "    return noise_removed\n",
        "\n",
        "#(2) Remove numbers.\n",
        "def remove_num (list):\n",
        "    num_removed  = [word for word in list if not word.isnumeric()]\n",
        "    return num_removed\n",
        "\n",
        " # (3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "def remove_stop (list):\n",
        "    stopword_removed = [word for word in list if word not in stopwords.words('english')]\n",
        "    return stopword_removed\n",
        "\n",
        "#(4) Lowercase all texts\n",
        "def lowercase (list):\n",
        "    lowercase = [word.lower() for word in list]\n",
        "    return lowercase\n",
        "# (5) Stemming\n",
        "def stemming (list):\n",
        "    ps = PorterStemmer()\n",
        "    word_stemmed = [ps.stem(word) for word in list]\n",
        "    return word_stemmed\n",
        "# (6) Lemmatization.\n",
        "def lemma (list):\n",
        "    wnl =  WordNetLemmatizer ()\n",
        "    word_lemmatized = [wnl.lemmatize(word) for word in list]\n",
        "    return word_lemmatized\n",
        "\n",
        "# Cleaning title reviews and content review\n",
        "cleaned_title = [cleaning_text (sublist) for sublist in tokens_title]\n",
        "cleaned_content = [cleaning_text (sublist) for sublist in tokens_content]\n",
        "# Adding cleaned texts to new columns of dataframe\n",
        "dict.update ({\"cleaned_review_title\":cleaned_title, \"cleaned_review_content\": cleaned_content })\n",
        "df = pd.DataFrame(dict)\n",
        "df\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "#(1) Tag arts of Speech (POS) and  calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb)...\n",
        "\n",
        "from collections import Counter\n",
        "for tokens in cleaned_title:\n",
        "    title_tagged = nltk.pos_tag(tokens)\n",
        "    #fd_title = nltk.FreqDist(tag for (token, tag) in title_tagged)\n",
        "    #print(fd_title)\n",
        "    title_tagged_counts = Counter (tag for tokens, tag in title_tagged)\n",
        "    print(title_tagged_counts)\n",
        "\n",
        "for tokens in cleaned_content:\n",
        "    content_tagged = nltk.pos_tag(tokens)\n",
        "    #fd_title = nltk.FreqDist(tag for (token, tag) in title_tagged)\n",
        "    #print(fd_title)\n",
        "    content_tagged_counts = Counter (tag for tokens, tag in content_tagged)\n",
        "    print(content_tagged_counts)\n",
        "  \n",
        "# (2)Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    }
  ]
}